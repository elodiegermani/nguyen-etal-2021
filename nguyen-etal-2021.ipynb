{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2224421a",
   "metadata": {},
   "source": [
    "# Replication: Nguyen *et al*, 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37e705b",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7667917e",
   "metadata": {},
   "source": [
    "This notebook attempts to reproduce the following paper (which already uses the [PPMI](http://ppmi-info.org) dataset):\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "Nguyen KP, et al. <a href=https://doi:10.1016/j.parkreldis.2021.02.026>Predicting Parkinson's disease trajectory using clinical and neuroimaging baseline measures.</a> Parkinsonism Relat Disord. 2021;85:44-51. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ae37a4",
   "metadata": {},
   "source": [
    "This study uses data from 82 PD subjects with rs-fMRI and MDS-UPDRS total score, encompassing both motor and non-motor symptomatology at the same visit. Of these 82 subjects, 53 subjects also had scores available at year 1 after imaging, 45 at year 2, and 33 at year 4. \n",
    "\n",
    "The fMRI data were acquired at resting-state on 3T scanners with the same acquisition parameters. Acquisition parameters are described below (table extracted from the original paper supplementary materials).\n",
    "\n",
    "<img src=\"images/acquisition.png\" width=800/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6478fb8",
   "metadata": {},
   "source": [
    "The demographics parameters for the PD patients were as follows (table extracted from the paper):\n",
    "\n",
    "<img src=\"images/demographics.png\" width=800/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5fcb7da",
   "metadata": {},
   "source": [
    "The main goal of this paper is to use imaging features extracted from rs-fMRI data and demographic features to train machine learning models to predict MDS-UPDRS scores of PD patients.\n",
    "\n",
    "Imaging features includes fractional Amplitude of Low Frequency Fluctuations (fALFF) and Regional Homogeneity (ReHo) averaged for different Regions of Interest (ROI) of the brain extracted from different atlases: [100-ROI Schaefer functional brain parcellation](https://doi.org/10.1093/cercor/bhx179), modified with an additional 35 striatal and cerebellar ROIs, 197-ROI and 444-ROI versions of the [Bootstrap Analysis of Stable Clusters (BASC197) atlas](https://doi.org/10.1016/j.neuroimage.2010.02.082).\n",
    "\n",
    "Different machine learning models were compared: ElasticNet regression, Support Vector Machine (SVM) with a linear kernel, Random Forest with a decision tree kernel, and Gradient Boosting with a decision tree kernel. \n",
    "\n",
    "An unbiased random search was conducted to optimize the hyperparameters of each model, including regularization strength and learning rate. To determine the best-performing parcellation, hyperparameter, and model combination for each target, a rigorous nested cross-validation approach was applied, with leave-one-out cross-validation (LOOCV) as the outer loop and 10-fold cross-validation as the inner loop."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6521618e",
   "metadata": {},
   "source": [
    "ReHo features explained 30.4%, 45.3%, 47.1%, and 25.5% of the variance in baseline, year 1, year 2, and year 4 MDS-UPDRS score, respectively. fALFF features explained 24.2%, 55.8%, 46.3%, and 15.2% of the variance in baseline, year 1, year 2, and year 4 MDS-UPDRS score, respectively. Results were significant at p = 0.001 (false discovery rate-corrected) at all timepoints except year 4, which was significant at p = 0.05.\n",
    "\n",
    "Results are displayed below (table extracted from the original paper). \n",
    "<img src=\"images/results.png\" width=800/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736e7cdf",
   "metadata": {},
   "source": [
    "## Initial setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2b29c5",
   "metadata": {},
   "source": [
    "We first initialize the notebook cache and install dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a211df6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This notebook was run on 2023-09-07 13:50:53 UTC +0000\n"
     ]
    }
   ],
   "source": [
    "import livingpark_utils\n",
    "\n",
    "utils = livingpark_utils.LivingParkUtils()\n",
    "utils.notebook_init()\n",
    "\n",
    "# import warnings filter\n",
    "from warnings import simplefilter\n",
    "# ignore all future warnings\n",
    "simplefilter(action='ignore', category=FutureWarning)\n",
    "simplefilter(action='ignore', category=UserWarning)\n",
    "simplefilter(action='ignore', category=RuntimeWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f997018a",
   "metadata": {},
   "source": [
    "## PPMI cohort preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d0ec03",
   "metadata": {},
   "source": [
    "We will build a PPMI cohort that matches the one used in the original study (Table 1) as closely as possible. Our cohort will be built directly from PPMI Study Data files so that it can be replicated and updated whenever necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d247d60",
   "metadata": {},
   "source": [
    "### Study data download"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2fcd2f5",
   "metadata": {},
   "source": [
    "We will start by downloading the PPMI Study Data files required to build our cohort: \n",
    "\n",
    "* Participant status (Parkinson's disease, healthy control, etc.)\n",
    "* Demographics\n",
    "* Age at visit\n",
    "* Clinical/cognitive assessment results:\n",
    "    * Montreal Cognitive Assessment (MoCA)\n",
    "    * Unified Parkinson's Disease Rating Scale (UPDRS) Parts I, II and III\n",
    "    * Geriatric Depression Scale (GDS)\n",
    "    * Hoehn-Yahr stage\n",
    "\n",
    "We will use the LivingPark utils library to download these files from the notebook. If files are already present in the notebook cache, they won't be downloaded again. Otherwise, a PPMI username and password are required to obtain the files. New PPMI accounts can be requested [here](http://ppmi-info.org)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e61d5541",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This notebook was run on 2023-09-07 13:50:53 UTC +0000\n",
      "This notebook was run on 2023-09-07 13:50:54 UTC +0000\n",
      "Download skipped: No missing files!\n",
      "File downloaded\n",
      "Removed 76 records where PDSTATE=ON and EXAMTM<PDMEDTM\n",
      "Number of removed records: 1\n",
      "Number of removed records: 60\n",
      "Number of removed records: 10\n",
      "Cleaned file saved in MDS_UPDRS_Part_III_clean.csv\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "from nguyenetal.constants import (\n",
    "    FILENAME_PARTICIPANT_STATUS,\n",
    "    FILENAME_DEMOGRAPHICS,\n",
    "    FILENAME_PD_HISTORY,\n",
    "    FILENAME_SOCIO,\n",
    "    FILENAME_AGE,\n",
    "    FILENAME_MOCA,\n",
    "    FILENAME_UPDRS1A,\n",
    "    FILENAME_UPDRS1B,\n",
    "    FILENAME_UPDRS2,\n",
    "    FILENAME_UPDRS3,\n",
    "    FILENAME_UPDRS3_CLEAN,\n",
    "    FILENAME_UPDRS4,\n",
    "    FILENAME_GDS,\n",
    "    FILENAME_FMRI_INFO,\n",
    "    FILENAME_FMRI_INFO_ZIP,\n",
    "    FILENAME_FMRI_METADATA\n",
    ")\n",
    "\n",
    "from nguyenetal.constants import (\n",
    "    COL_PAT_ID,\n",
    "    COL_VISIT_TYPE,\n",
    "    COL_STATUS,\n",
    "    COL_PD_STATE,\n",
    "    COL_AGE,\n",
    "    COL_SEX,\n",
    "    COL_EDUCATION,\n",
    "    COL_UPDRS3,\n",
    "    COL_UPDRS1A,\n",
    "    COL_UPDRS1B,\n",
    "    COL_UPDRS1,\n",
    "    COL_UPDRS2,\n",
    "    COL_UPDRS4,\n",
    "    COL_MOCA,\n",
    ")\n",
    "\n",
    "from nguyenetal.constants import (\n",
    "    COL_DATE_INFO,\n",
    "    COL_DATE_BIRTH,\n",
    "    COL_DATE_PD,\n",
    "    FIELD_STRENGTH,\n",
    "    STATUS_PD\n",
    "    \n",
    ")\n",
    "\n",
    "from nguyenetal.constants import (\n",
    "    COL_IMAGING_PROTOCOL,\n",
    "    COLS_DATE, \n",
    "    IDA_STATUS_MAP,\n",
    "    IDA_COLNAME_MAP,\n",
    "    IDA_VISIT_MAP,\n",
    "    STATUS_MED,\n",
    ")\n",
    "\n",
    "from nguyenetal.utils.cohort_utils import (\n",
    "    load_ppmi_csv,\n",
    "    get_fMRI_cohort,\n",
    "    mean_impute,\n",
    "    compute_summary_features,\n",
    "    get_features,\n",
    "    get_threshold, \n",
    "    get_outcome_measures\n",
    ")\n",
    "\n",
    "from livingpark_utils.scripts import pd_status\n",
    "from livingpark_utils.dataset.ppmi import disease_duration\n",
    "\n",
    "from functools import reduce\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "pd.set_option('display.max_rows', 500)\n",
    "import datetime as dt\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "#sns.set_palette(\"rocket\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8294b49",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download skipped: No missing files!\n"
     ]
    }
   ],
   "source": [
    "from livingpark_utils.download.ppmi import Downloader\n",
    "\n",
    "required_files = [\n",
    "    FILENAME_PARTICIPANT_STATUS,\n",
    "    FILENAME_PD_HISTORY,\n",
    "    FILENAME_DEMOGRAPHICS,\n",
    "    FILENAME_SOCIO,\n",
    "    FILENAME_AGE,\n",
    "    FILENAME_MOCA,\n",
    "    FILENAME_UPDRS1A,\n",
    "    FILENAME_UPDRS1B,\n",
    "    FILENAME_UPDRS2,\n",
    "    FILENAME_UPDRS3,\n",
    "    FILENAME_UPDRS4,\n",
    "    FILENAME_GDS\n",
    "]\n",
    "\n",
    "downloader = Downloader(utils.study_files_dir)\n",
    "utils.get_study_files(required_files, default=downloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da2be476-42a0-47cc-8fe7-ae2d097b72e6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import ppmi_downloader\n",
    "import os\n",
    "mri_file_name = \"fMRI_info.csv\"\n",
    "if not os.path.exists(os.path.join(utils.study_files_dir, mri_file_name)):\n",
    "    ppmi = ppmi_downloader.PPMIDownloader()\n",
    "    file_name = ppmi.download_fmri_info(destination_dir=utils.study_files_dir)\n",
    "    os.rename(\n",
    "        os.path.join(utils.study_files_dir, file_name),\n",
    "        os.path.join(utils.study_files_dir, mri_file_name),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15db25f7",
   "metadata": {},
   "source": [
    "The main cohort contains 82 participants from either the `Parkinson's Disease` or the `GenCohort PD` cohort of PPMI. These 82 participants have rs-fMRI and outcome scores at the same visit available. MDS-UPDRS scores included the Part III Motor Examination conducted on-medication. Off-medication scores were not used due to unavailability for over half of the subjects and because examinations are more practically conducted on-medication in the clinic. \n",
    "\n",
    "We selected participants that:\n",
    "* belonged to the selected cohort\n",
    "* had fMRI scans with same acquisition parameters as those used in the paper."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4751e54b",
   "metadata": {},
   "source": [
    "### Participants with fMRI data available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95a43fea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============== fMRI cohort ===============\n",
      "Using fMRI info file: /Users/egermani/Documents/nguyen-etal-2021/inputs/study_files/fMRI_info.csv\n",
      "Dropping 258 subjects with non-integer IDs\n",
      "WARNING: Duplicate subjects in cohort\n",
      "COHORT_DEFINITION\n",
      "Parkinson's Disease    118\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(f\"=============== fMRI cohort ===============\")\n",
    "df_status = load_ppmi_csv(utils, FILENAME_PARTICIPANT_STATUS)\n",
    "df_fMRI_subset = get_fMRI_cohort(utils)\n",
    "\n",
    "# cohort composition: number of PD patients/healthy controls\n",
    "print(\n",
    "    df_status.loc[\n",
    "        df_status[COL_PAT_ID].isin(df_fMRI_subset[COL_PAT_ID]), COL_STATUS\n",
    "    ].value_counts()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f31a9e3",
   "metadata": {},
   "source": [
    "Then, we load/compute and merge all the required clinical/cognitive measures:\n",
    "* UPDRS Part I\n",
    "* UPDRS Part II\n",
    "* UPDRS Part III\n",
    "* UPDRS Part IV\n",
    "* MoCA\n",
    "* GDS Score\n",
    "\n",
    "Missing values are imputed with the mean across the entire dataset, except for the UPDRS Part III score (handled below).\n",
    "\n",
    "There are two files associated with UPDRS Part I (IA: Complex behaviors; IB: Partipant questionnaire). We use the sum of the total score in each file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "83770631",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_for_merge = [COL_PAT_ID, COL_DATE_INFO, COL_VISIT_TYPE]\n",
    "\n",
    "# Load necessary files\n",
    "df_updrs1a = load_ppmi_csv(utils, FILENAME_UPDRS1A, convert_int = [COL_UPDRS1A], cols_to_impute=COL_UPDRS1A)\n",
    "df_updrs1b = load_ppmi_csv(utils, FILENAME_UPDRS1B, convert_int = [COL_UPDRS1B], cols_to_impute=COL_UPDRS1B)\n",
    "df_updrs2 = load_ppmi_csv(utils, FILENAME_UPDRS2, convert_int = [COL_UPDRS2], cols_to_impute=COL_UPDRS2)\n",
    "df_updrs3 = load_ppmi_csv(utils, FILENAME_UPDRS3_CLEAN, convert_int = [COL_UPDRS3])\n",
    "df_updrs4 = load_ppmi_csv(utils, FILENAME_UPDRS4, convert_int = [COL_UPDRS4], cols_to_impute=COL_UPDRS4)\n",
    "\n",
    "df_moca = load_ppmi_csv(utils, FILENAME_MOCA, convert_int = [COL_MOCA])\n",
    "df_gds = load_ppmi_csv(utils, FILENAME_GDS)\n",
    "\n",
    "# Sum UPDRS IA and IB scores\n",
    "df_updrs1 = df_updrs1a.merge(df_updrs1b, on=cols_for_merge)\n",
    "df_updrs1[COL_UPDRS1] = df_updrs1.loc[:, [COL_UPDRS1A, COL_UPDRS1B]].sum(axis=\"columns\")\n",
    "\n",
    "# Drop unused UPDRSIII scores (only ON medication)\n",
    "df_updrs3 = df_updrs3.drop(df_updrs3.index[(df_updrs3['PAG_NAME'] == 'NUPDRS3') & \\\n",
    "                         (df_updrs3['EVENT_ID'].isin(['V04', 'V06', 'V08', 'V10', 'V12', 'V13', 'V15']))])\n",
    "#df_updrs3 = df_updrs3.drop(df_updrs3.index[(df_updrs3['PAG_NAME'] == 'NUPDR3OF')])\n",
    "#df_updrs3 = df_updrs3.drop(df_updrs3.index[(df_updrs3['PAG_NAME'] == 'NUPDR3ON')])\n",
    "\n",
    "# Select UPDRS columns to merge\n",
    "df_updrs1 = df_updrs1.loc[:, cols_for_merge + [COL_UPDRS1]]\n",
    "df_updrs2 = df_updrs2.loc[:, cols_for_merge + [COL_UPDRS2]]\n",
    "df_updrs3 = df_updrs3.loc[:, cols_for_merge + [COL_UPDRS3, 'NHY', 'PAG_NAME', 'PDSTATE']]\n",
    "df_updrs4 = df_updrs4.loc[:, cols_for_merge + [COL_UPDRS4]]\n",
    "\n",
    "# Compute GDS total score \n",
    "gds_cols = df_gds.columns[['GDS' in strcol for strcol in df_gds.columns]].tolist()\n",
    "df_gds['GDS_TOTAL'] = df_gds[gds_cols].sum(axis=1)\n",
    "df_gds = df_gds.loc[:, cols_for_merge + ['GDS_TOTAL']]\n",
    "\n",
    "# Select MOCA columns to merge\n",
    "df_moca = df_moca.loc[:, cols_for_merge + [COL_MOCA]]\n",
    "\n",
    "# Merge \n",
    "df_assessments_all = reduce(\n",
    "    lambda df1, df2: df1.merge(df2, on=cols_for_merge, how=\"outer\"),\n",
    "    [df_updrs2, df_updrs3, df_updrs1, df_updrs4, df_moca, df_gds],\n",
    ").drop_duplicates()\n",
    "\n",
    "# Compute TOTAL UPDRS SCORE \n",
    "updrs_cols = [COL_UPDRS1, COL_UPDRS2, COL_UPDRS3, COL_UPDRS4]\n",
    "df_assessments_all['UPDRS_TOT'] = df_assessments_all[updrs_cols].sum(axis=1)\n",
    "   \n",
    "# Only keep cohort participants\n",
    "df_cohort_assessments = df_assessments_all.loc[\n",
    "    df_assessments_all[COL_PAT_ID].isin(df_fMRI_subset[COL_PAT_ID])]\n",
    "\n",
    "# Drop participants that don't have UPDRS III Score\n",
    "df_cohort_assessments = df_cohort_assessments.dropna(subset=['NP3TOT'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985fd1ea",
   "metadata": {},
   "source": [
    "Only participants with outcome score (UPDRS) and rs-fMRI data at the same visit were used, so we filter both datasets to keep only participants sessions that have both. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4139322",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fMRI_cohort = pd.DataFrame()\n",
    "for i in range(len(df_cohort_assessments)):\n",
    "    df_fMRI_cohort = pd.concat([df_fMRI_cohort, \n",
    "                        df_fMRI_subset[df_fMRI_subset[COL_PAT_ID] == df_cohort_assessments.iloc[i][COL_PAT_ID]]\\\n",
    "                        [df_fMRI_subset[COL_VISIT_TYPE] == df_cohort_assessments.iloc[i][COL_VISIT_TYPE]]]\n",
    "                        )\n",
    "    \n",
    "df_scores_cohort = pd.DataFrame()\n",
    "for i in range(len(df_fMRI_subset)):\n",
    "    df_scores_cohort = pd.concat([df_scores_cohort, \n",
    "                    df_cohort_assessments[df_cohort_assessments[COL_PAT_ID] == df_fMRI_subset.iloc[i][COL_PAT_ID]]\\\n",
    "                    [df_cohort_assessments[COL_VISIT_TYPE] == df_fMRI_subset.iloc[i][COL_VISIT_TYPE]]]\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c52350d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fMRI_cols_to_include = ['PATNO', 'Sex','COHORT_DEFINITION','EVENT_ID', 'INFODT', 'Age', \n",
    "                        'Description', 'Imaging Protocol', 'Image ID']\n",
    "scores_cols_to_include = ['PATNO', 'EVENT_ID','PAG_NAME' ,'NP2PTOT', 'NP3TOT', 'NP1RTOT+NP1PTOT',\n",
    "       'NP4TOT', 'NHY','MCATOT', 'GDS_TOTAL', 'UPDRS_TOT']\n",
    "\n",
    "df_fMRI_cohort = df_fMRI_cohort.loc[:, fMRI_cols_to_include]\n",
    "df_scores_cohort = df_scores_cohort.loc[:, scores_cols_to_include]\n",
    "\n",
    "# Merge important columns from both datasets\n",
    "df_global_cohort = df_fMRI_cohort.merge(df_scores_cohort, on=[COL_PAT_ID, COL_VISIT_TYPE])\n",
    "df_global_cohort = df_global_cohort.sort_values(by=['PATNO','INFODT'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b8fe53",
   "metadata": {},
   "source": [
    "### Baseline cohort"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af45a9e0",
   "metadata": {},
   "source": [
    "For the training, authors used the first scan & outcome score available for each participant. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "978b8ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_global_cohort_baseline = df_global_cohort.drop_duplicates(subset=COL_PAT_ID)\n",
    "df_global_cohort_baseline = df_global_cohort_baseline[\n",
    "    df_global_cohort_baseline[COL_DATE_INFO] < pd.Timestamp(2020, 1, 1, 12)\n",
    "    ] # Removed due to the date of the study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da8939ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of participants selected using papers informations: 102\n"
     ]
    }
   ],
   "source": [
    "print('Number of participants selected using papers informations:', len(df_global_cohort_baseline))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33877567",
   "metadata": {},
   "source": [
    "### Prediction cohort "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2692656f",
   "metadata": {},
   "source": [
    "In the paper, authors are trying to predict UPDRS scores at baseline (same session as fMRI data), 1 year after, 2 years after and 4 years after. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6089e87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DF with outcome scores for every participants\n",
    "df_global_cohort_pred = df_cohort_assessments[df_cohort_assessments\\\n",
    "                                              [COL_PAT_ID].isin(df_global_cohort_baseline[COL_PAT_ID].tolist())]\n",
    "\n",
    "# Filter by date due to the date of publication of the paper. \n",
    "df_global_cohort_pred = df_global_cohort_pred[df_global_cohort_pred[COL_DATE_INFO] < pd.Timestamp(2020, 1, 1, 12)]\n",
    "\n",
    "# Event taken as Baseline\n",
    "df_global_cohort_pred['BASELINE_EV'] = [df_global_cohort_baseline[COL_VISIT_TYPE]\\\n",
    "                [df_global_cohort_baseline[COL_PAT_ID] == df_global_cohort_pred[COL_PAT_ID].iloc[i]].iloc[0] \\\n",
    "                             for i in range(len(df_global_cohort_pred))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "57eaf5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "eq_1year = {\n",
    "    'BL':'V04',\n",
    "    'ST':'V04',\n",
    "    'V04':'V06',\n",
    "    'V06':'V08',\n",
    "    'V08':'V10',\n",
    "    'V10':'V12'\n",
    "}\n",
    "\n",
    "eq_2year = {\n",
    "    'BL':'V06',\n",
    "    'ST':'V06',\n",
    "    'V04':'V08',\n",
    "    'V06':'V10',\n",
    "    'V08':'V12',\n",
    "    'V10':'V13'\n",
    "}\n",
    "\n",
    "eq_4year = {\n",
    "    'BL':'V10',\n",
    "    'ST':'V10',\n",
    "    'V04':'V12',\n",
    "    'V06':'V13',\n",
    "    'V08':'V14',\n",
    "    'V10':'V15'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0c48024d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_global_cohort_pred['1Y_EVENT'] = [eq_1year[b] for b in df_global_cohort_pred['BASELINE_EV'].tolist()]\n",
    "df_global_cohort_pred['2Y_EVENT'] = [eq_2year[b] for b in df_global_cohort_pred['BASELINE_EV'].tolist()]\n",
    "df_global_cohort_pred['4Y_EVENT'] = [eq_4year[b] for b in df_global_cohort_pred['BASELINE_EV'].tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cdff5e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_global_1y = df_global_cohort_pred[\n",
    "        df_global_cohort_pred[COL_VISIT_TYPE]==df_global_cohort_pred['1Y_EVENT']\n",
    "    ].drop_duplicates(subset=COL_PAT_ID)\n",
    "\n",
    "df_global_1y = df_global_1y.merge(df_global_cohort_baseline[['PATNO','Sex']], on=['PATNO'])\n",
    "\n",
    "df_global_2y = df_global_cohort_pred[\n",
    "        df_global_cohort_pred[COL_VISIT_TYPE]==df_global_cohort_pred['2Y_EVENT']\n",
    "    ].drop_duplicates(subset=COL_PAT_ID)\n",
    "\n",
    "df_global_2y = df_global_2y.merge(df_global_cohort_baseline[['PATNO','Sex']], on=['PATNO'])\n",
    "\n",
    "df_global_4y = df_global_cohort_pred[\n",
    "        df_global_cohort_pred[COL_VISIT_TYPE]==df_global_cohort_pred['4Y_EVENT']\n",
    "    ].drop_duplicates(subset=COL_PAT_ID)\n",
    "\n",
    "df_global_4y = df_global_4y.merge(df_global_cohort_baseline[['PATNO','Sex']], on=['PATNO'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3e9d0b4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of participants for 1y cohort:  63\n",
      "Number of participants for 2y cohort:  68\n",
      "Number of participants for 4y cohort:  47\n"
     ]
    }
   ],
   "source": [
    "print('Number of participants for 1y cohort: ', len(df_global_1y))\n",
    "print('Number of participants for 2y cohort: ', len(df_global_2y))\n",
    "print('Number of participants for 4y cohort: ', len(df_global_4y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0991609e",
   "metadata": {},
   "source": [
    "### Global cohort"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f911fcec",
   "metadata": {},
   "source": [
    "Using the information that we had in the paper, we were able to select 105 participants against 82 mentioned in the paper. We create a demographics table similar to the one in the original paper to verify our cohort. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9a2c107b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download skipped: No missing files!\n",
      "Download skipped: No missing files!\n",
      "Download skipped: No missing files!\n",
      "Download skipped: No missing files!\n",
      "Download skipped: No missing files!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"2\" halign=\"left\">Baseline</th>\n",
       "      <th colspan=\"2\" halign=\"left\">Year 1</th>\n",
       "      <th colspan=\"2\" halign=\"left\">Year 2</th>\n",
       "      <th colspan=\"2\" halign=\"left\">Year 4</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>Original</th>\n",
       "      <th>Replication</th>\n",
       "      <th>Original</th>\n",
       "      <th>Replication</th>\n",
       "      <th>Original</th>\n",
       "      <th>Replication</th>\n",
       "      <th>Original</th>\n",
       "      <th>Replication</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>% Caucasian</th>\n",
       "      <td>95.1</td>\n",
       "      <td>95.1</td>\n",
       "      <td>94.4</td>\n",
       "      <td>95.2</td>\n",
       "      <td>97.8</td>\n",
       "      <td>97.1</td>\n",
       "      <td>97.0</td>\n",
       "      <td>97.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>% African-American</th>\n",
       "      <td>2.4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.9</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>% Asian</th>\n",
       "      <td>3.7</td>\n",
       "      <td>2.9</td>\n",
       "      <td>5.6</td>\n",
       "      <td>3.2</td>\n",
       "      <td>4.4</td>\n",
       "      <td>2.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>% Hispanic</th>\n",
       "      <td>1.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>% Male</th>\n",
       "      <td>67.0</td>\n",
       "      <td>66.7</td>\n",
       "      <td>68.5</td>\n",
       "      <td>65.1</td>\n",
       "      <td>82.2</td>\n",
       "      <td>76.5</td>\n",
       "      <td>75.8</td>\n",
       "      <td>70.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>% right-handed</th>\n",
       "      <td>89.0</td>\n",
       "      <td>89.2</td>\n",
       "      <td>85.2</td>\n",
       "      <td>87.3</td>\n",
       "      <td>88.9</td>\n",
       "      <td>91.2</td>\n",
       "      <td>87.9</td>\n",
       "      <td>87.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mean age, years</th>\n",
       "      <td>62.1 ± 9.8</td>\n",
       "      <td>62.1 ± 9.5</td>\n",
       "      <td>61.9 ± 10.3</td>\n",
       "      <td>62.5 ± 9.7</td>\n",
       "      <td>63.6 ± 9.2</td>\n",
       "      <td>63.5 ± 9.8</td>\n",
       "      <td>59.5 ± 11.0</td>\n",
       "      <td>65.7 ± 9.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mean years of education</th>\n",
       "      <td>15.6 ± 3.0</td>\n",
       "      <td>15.6 ± 2.8</td>\n",
       "      <td>15.1 ± 3.2</td>\n",
       "      <td>15.1 ± 2.6</td>\n",
       "      <td>15.1 ± 3.3</td>\n",
       "      <td>15.3 ± 2.7</td>\n",
       "      <td>15.0 ± 3.4</td>\n",
       "      <td>15.4 ± 3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mean disease duration at baseline, days</th>\n",
       "      <td>770 ± 565</td>\n",
       "      <td>870.4 ± 597.1</td>\n",
       "      <td>808 ± 576</td>\n",
       "      <td>998.3 ± 593.1</td>\n",
       "      <td>771 ± 506</td>\n",
       "      <td>902.9 ± 566.4</td>\n",
       "      <td>532 ± 346</td>\n",
       "      <td>743.7 ± 591.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mean MDS-UPDRS at baseline</th>\n",
       "      <td>33.9 ± 15.8</td>\n",
       "      <td>34.5 ± 15.6</td>\n",
       "      <td>38.0 ± 20.9</td>\n",
       "      <td>33.7 ± 15.3</td>\n",
       "      <td>40.2 ± 18.2</td>\n",
       "      <td>34.0 ± 15.2</td>\n",
       "      <td>34.9 ± 15.7</td>\n",
       "      <td>31.7 ± 14.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mean MDS-UPDRS at timepoint</th>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>39.2 ± 21.6</td>\n",
       "      <td>39.5 ± 22.2</td>\n",
       "      <td>40.9 ± 18.5</td>\n",
       "      <td>40.7 ± 19.7</td>\n",
       "      <td>35.9 ± 16.5</td>\n",
       "      <td>41.9 ± 21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mean MoCA at baseline</th>\n",
       "      <td>26.7 ± 2.8</td>\n",
       "      <td>26.5 ± 3.0</td>\n",
       "      <td>26.9 ± 3.2</td>\n",
       "      <td>27.0 ± 2.9</td>\n",
       "      <td>26.7 ± 3.5</td>\n",
       "      <td>26.9 ± 2.4</td>\n",
       "      <td>27.5 ± 2.3</td>\n",
       "      <td>26.7 ± 3.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mean GDS at Baseline</th>\n",
       "      <td>5.4 ± 1.4</td>\n",
       "      <td>5.4 ± 1.4</td>\n",
       "      <td>5.4 ± 1.6</td>\n",
       "      <td>5.5 ± 1.7</td>\n",
       "      <td>5.4 ± 1.2</td>\n",
       "      <td>5.5 ± 1.4</td>\n",
       "      <td>5.4 ± 1.7</td>\n",
       "      <td>5.8 ± 1.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mean Hoehn-Yahr stage</th>\n",
       "      <td>1.8 ± 0.5</td>\n",
       "      <td>1.7 ± 0.5</td>\n",
       "      <td>1.8 ± 0.5</td>\n",
       "      <td>1.7 ± 0.5</td>\n",
       "      <td>1.8 ± 0.5</td>\n",
       "      <td>1.8 ± 0.5</td>\n",
       "      <td>1.7 ± 0.5</td>\n",
       "      <td>1.9 ± 0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Number of subject</th>\n",
       "      <td>82</td>\n",
       "      <td>102</td>\n",
       "      <td>53</td>\n",
       "      <td>63</td>\n",
       "      <td>45</td>\n",
       "      <td>68</td>\n",
       "      <td>33</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Baseline                 \\\n",
       "                                            Original    Replication   \n",
       "% Caucasian                                     95.1           95.1   \n",
       "% African-American                               2.4            2.0   \n",
       "% Asian                                          3.7            2.9   \n",
       "% Hispanic                                       1.2            1.0   \n",
       "% Male                                          67.0           66.7   \n",
       "% right-handed                                  89.0           89.2   \n",
       "Mean age, years                           62.1 ± 9.8     62.1 ± 9.5   \n",
       "Mean years of education                   15.6 ± 3.0     15.6 ± 2.8   \n",
       "Mean disease duration at baseline, days    770 ± 565  870.4 ± 597.1   \n",
       "Mean MDS-UPDRS at baseline               33.9 ± 15.8    34.5 ± 15.6   \n",
       "Mean MDS-UPDRS at timepoint                        -              -   \n",
       "Mean MoCA at baseline                     26.7 ± 2.8     26.5 ± 3.0   \n",
       "Mean GDS at Baseline                       5.4 ± 1.4      5.4 ± 1.4   \n",
       "Mean Hoehn-Yahr stage                      1.8 ± 0.5      1.7 ± 0.5   \n",
       "Number of subject                                 82            102   \n",
       "\n",
       "                                              Year 1                 \\\n",
       "                                            Original    Replication   \n",
       "% Caucasian                                     94.4           95.2   \n",
       "% African-American                               1.9            1.6   \n",
       "% Asian                                          5.6            3.2   \n",
       "% Hispanic                                         0            1.6   \n",
       "% Male                                          68.5           65.1   \n",
       "% right-handed                                  85.2           87.3   \n",
       "Mean age, years                          61.9 ± 10.3     62.5 ± 9.7   \n",
       "Mean years of education                   15.1 ± 3.2     15.1 ± 2.6   \n",
       "Mean disease duration at baseline, days    808 ± 576  998.3 ± 593.1   \n",
       "Mean MDS-UPDRS at baseline               38.0 ± 20.9    33.7 ± 15.3   \n",
       "Mean MDS-UPDRS at timepoint              39.2 ± 21.6    39.5 ± 22.2   \n",
       "Mean MoCA at baseline                     26.9 ± 3.2     27.0 ± 2.9   \n",
       "Mean GDS at Baseline                       5.4 ± 1.6      5.5 ± 1.7   \n",
       "Mean Hoehn-Yahr stage                      1.8 ± 0.5      1.7 ± 0.5   \n",
       "Number of subject                                 53             63   \n",
       "\n",
       "                                              Year 2                 \\\n",
       "                                            Original    Replication   \n",
       "% Caucasian                                     97.8           97.1   \n",
       "% African-American                                 0            0.0   \n",
       "% Asian                                          4.4            2.9   \n",
       "% Hispanic                                         0            1.5   \n",
       "% Male                                          82.2           76.5   \n",
       "% right-handed                                  88.9           91.2   \n",
       "Mean age, years                           63.6 ± 9.2     63.5 ± 9.8   \n",
       "Mean years of education                   15.1 ± 3.3     15.3 ± 2.7   \n",
       "Mean disease duration at baseline, days    771 ± 506  902.9 ± 566.4   \n",
       "Mean MDS-UPDRS at baseline               40.2 ± 18.2    34.0 ± 15.2   \n",
       "Mean MDS-UPDRS at timepoint              40.9 ± 18.5    40.7 ± 19.7   \n",
       "Mean MoCA at baseline                     26.7 ± 3.5     26.9 ± 2.4   \n",
       "Mean GDS at Baseline                       5.4 ± 1.2      5.5 ± 1.4   \n",
       "Mean Hoehn-Yahr stage                      1.8 ± 0.5      1.8 ± 0.5   \n",
       "Number of subject                                 45             68   \n",
       "\n",
       "                                              Year 4                 \n",
       "                                            Original    Replication  \n",
       "% Caucasian                                     97.0           97.9  \n",
       "% African-American                                 0            0.0  \n",
       "% Asian                                          3.0            2.1  \n",
       "% Hispanic                                         0            0.0  \n",
       "% Male                                          75.8           70.2  \n",
       "% right-handed                                  87.9           87.2  \n",
       "Mean age, years                          59.5 ± 11.0     65.7 ± 9.7  \n",
       "Mean years of education                   15.0 ± 3.4     15.4 ± 3.0  \n",
       "Mean disease duration at baseline, days    532 ± 346  743.7 ± 591.8  \n",
       "Mean MDS-UPDRS at baseline               34.9 ± 15.7    31.7 ± 14.4  \n",
       "Mean MDS-UPDRS at timepoint              35.9 ± 16.5    41.9 ± 21.6  \n",
       "Mean MoCA at baseline                     27.5 ± 2.3     26.7 ± 3.2  \n",
       "Mean GDS at Baseline                       5.4 ± 1.7      5.8 ± 1.7  \n",
       "Mean Hoehn-Yahr stage                      1.7 ± 0.5      1.9 ± 0.5  \n",
       "Number of subject                                 33             47  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_allyears_summary = pd.DataFrame(columns = [('Baseline', 'Original'), ('Baseline', 'Replication'),\n",
    "                                              ('Year 1', 'Original'), ('Year 1', 'Replication'), \n",
    "                                              ('Year 2', 'Original'), ('Year 2', 'Replication'), \n",
    "                                              ('Year 4', 'Original'), ('Year 4', 'Replication')])\n",
    "\n",
    "df_allyears_summary[('Baseline', 'Original')]=['95.1','2.4','3.7','1.2','67.0','89.0','62.1 ± 9.8',\n",
    " '15.6 ± 3.0','770 ± 565','33.9 ± 15.8','-','26.7 ± 2.8','5.4 ± 1.4','1.8 ± 0.5']\n",
    "\n",
    "df_allyears_summary[('Year 1', 'Original')] = ['94.4','1.9','5.6','0','68.5','85.2','61.9 ± 10.3',\n",
    " '15.1 ± 3.2','808 ± 576','38.0 ± 20.9','39.2 ± 21.6','26.9 ± 3.2','5.4 ± 1.6','1.8 ± 0.5']\n",
    "\n",
    "df_allyears_summary[('Year 2', 'Original')] = ['97.8','0','4.4','0','82.2','88.9','63.6 ± 9.2',\n",
    " '15.1 ± 3.3','771 ± 506','40.2 ± 18.2','40.9 ± 18.5','26.7 ± 3.5','5.4 ± 1.2','1.8 ± 0.5']\n",
    "\n",
    "df_allyears_summary[('Year 4', 'Original')] = ['97.0','0','3.0','0','75.8','87.9','59.5 ± 11.0',\n",
    "    '15.0 ± 3.4','532 ± 346','34.9 ± 15.7','35.9 ± 16.5','27.5 ± 2.3','5.4 ± 1.7','1.7 ± 0.5']\n",
    "\n",
    "df_allyears_summary[('Baseline', 'Replication')] = compute_summary_features(df_global_cohort_baseline, utils,\n",
    "                                                                'baseline').tolist()\n",
    "df_allyears_summary[('Year 1', 'Replication')] = compute_summary_features(df_global_1y, utils,\n",
    "                                                                '1Y', df_global_cohort_baseline).tolist()\n",
    "df_allyears_summary[('Year 2', 'Replication')] = compute_summary_features(df_global_2y, utils,\n",
    "                                                                '2Y', df_global_cohort_baseline).tolist()\n",
    "df_allyears_summary[('Year 4', 'Replication')] = compute_summary_features(df_global_4y, utils,\n",
    "                                                                '4Y', df_global_cohort_baseline).tolist()\n",
    "\n",
    "df_allyears_summary.index = compute_summary_features(df_global_cohort_baseline, utils,\n",
    "                                                                'baseline').index\n",
    "\n",
    "df_allyears_summary.loc['Number of subject'] = [82, len(df_global_cohort_baseline), 53, len(df_global_1y), \n",
    "                                      45, len(df_global_2y), 33, len(df_global_4y)]\n",
    "\n",
    "df_allyears_summary.columns = pd.MultiIndex.from_tuples(df_allyears_summary.columns)\n",
    "df_allyears_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49365940",
   "metadata": {},
   "source": [
    "The values obtained in this table are basically similar to those in the paper, except in terms of number of participants. The original baseline cohort was composed of 82 participants compared to 102 in our case. Year 1, 2 and 4 cohorts also exhibit a larger number of participants than in the original paper. \n",
    "\n",
    "Mean values of demographics and clinical features are similar to those of the original paper. \n",
    "\n",
    "To obtain the same number of participants in each cohort, we will randomly sample the same number of participants as those used in the paper and perform the replication on these participants."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84bafd24-67c1-483d-91b7-4c2e5a24b774",
   "metadata": {},
   "source": [
    "### Final cohort: resampling to obtain similar sample sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7f16e39a-67d8-4b67-8eff-988f7e6a67f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# participants_1pred_scores = [sub for sub in df_global_cohort_baseline[COL_PAT_ID].tolist() \\\n",
    "#                            if (sub in df_global_1y[COL_PAT_ID].tolist()) or \\\n",
    "#                           (sub in df_global_2y[COL_PAT_ID].tolist()) or \\\n",
    "#                           (sub in df_global_4y[COL_PAT_ID].tolist())]\n",
    "\n",
    "# print('Number of participants with AT LEAST one prediction score:', len(participants_1pred_scores))\n",
    "\n",
    "# random_sampling = np.random.choice(participants_1pred_scores, size=102, replace=False)\n",
    "\n",
    "# print('Number of participant in random sampling for baseline:', len(random_sampling))\n",
    "\n",
    "# random_1y = [sub for sub in random_sampling if sub in df_global_1y[COL_PAT_ID].tolist()]\n",
    "# random_2y = [sub for sub in random_sampling if sub in df_global_2y[COL_PAT_ID].tolist()]\n",
    "# random_4y = [sub for sub in random_sampling if sub in df_global_4y[COL_PAT_ID].tolist()]\n",
    "\n",
    "# print('Number of participant in random sampling for 1y:', len(random_1y))\n",
    "# print('Number of participant in random sampling for 2y:', len(random_2y))\n",
    "# print('Number of participant in random sampling for 4y:', len(random_4y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9ec5f732-49a2-49be-ad40-86ace324b0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cohort_baseline = df_global_cohort_baseline\n",
    "df_cohort_1y = df_global_1y\n",
    "df_cohort_2y = df_global_2y\n",
    "df_cohort_4y = df_global_4y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b30a3ca8-c773-483a-8190-eeec666a299d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plot demographic features for this cohort\n",
    "# df_allyears_summary_cohort = pd.DataFrame(columns = [('Baseline', 'Original'), ('Baseline', 'Replication'),\n",
    "#                                               ('Year 1', 'Original'), ('Year 1', 'Replication'), \n",
    "#                                               ('Year 2', 'Original'), ('Year 2', 'Replication'), \n",
    "#                                               ('Year 4', 'Original'), ('Year 4', 'Replication')])\n",
    "\n",
    "# df_allyears_summary_cohort[('Baseline', 'Original')]=['95.1','2.4','3.7','1.2','67.0','89.0','62.1 ± 9.8',\n",
    "#  '15.6 ± 3.0','770 ± 565','33.9 ± 15.8','-','26.7 ± 2.8','5.4 ± 1.4','1.8 ± 0.5']\n",
    "\n",
    "# df_allyears_summary_cohort[('Year 1', 'Original')] = ['94.4','1.9','5.6','0','68.5','85.2','61.9 ± 10.3',\n",
    "#  '15.1 ± 3.2','808 ± 576','38.0 ± 20.9','39.2 ± 21.6','26.9 ± 3.2','5.4 ± 1.6','1.8 ± 0.5']\n",
    "\n",
    "# df_allyears_summary_cohort[('Year 2', 'Original')] = ['97.8','0','4.4','0','82.2','88.9','63.6 ± 9.2',\n",
    "#  '15.1 ± 3.3','771 ± 506','40.2 ± 18.2','40.9 ± 18.5','26.7 ± 3.5','5.4 ± 1.2','1.8 ± 0.5']\n",
    "\n",
    "# df_allyears_summary_cohort[('Year 4', 'Original')] = ['97.0','0','3.0','0','75.8','87.9','59.5 ± 11.0',\n",
    "#     '15.0 ± 3.4','532 ± 346','34.9 ± 15.7','35.9 ± 16.5','27.5 ± 2.3','5.4 ± 1.7','1.7 ± 0.5']\n",
    "\n",
    "# df_allyears_summary_cohort[('Baseline', 'Replication')] = compute_summary_features(df_cohort_baseline, utils,\n",
    "#                                                                 'baseline').tolist()\n",
    "# df_allyears_summary_cohort[('Year 1', 'Replication')] = compute_summary_features(df_cohort_1y, utils,\n",
    "#                                                                 '1Y', df_cohort_baseline).tolist()\n",
    "# df_allyears_summary_cohort[('Year 2', 'Replication')] = compute_summary_features(df_cohort_2y, utils,\n",
    "#                                                                 '2Y', df_cohort_baseline).tolist()\n",
    "# df_allyears_summary_cohort[('Year 4', 'Replication')] = compute_summary_features(df_cohort_4y, utils,\n",
    "#                                                                 '4Y', df_cohort_baseline).tolist()\n",
    "\n",
    "# df_allyears_summary_cohort.index = compute_summary_features(df_cohort_baseline, utils,\n",
    "#                                                                 'baseline').index\n",
    "\n",
    "# df_allyears_summary_cohort.loc['Number of subject'] = [82, len(df_cohort_baseline), 53, len(df_cohort_1y), \n",
    "#                                       45, len(df_cohort_2y), 33, len(df_cohort_4y)]\n",
    "\n",
    "# df_allyears_summary_cohort.columns = pd.MultiIndex.from_tuples(df_allyears_summary_cohort.columns)\n",
    "# df_allyears_summary_cohort"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ecbb1be",
   "metadata": {},
   "source": [
    "## Imaging features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "badf9f5c-8696-4d8b-a06b-7b26d6e121c5",
   "metadata": {},
   "source": [
    "We create a dataframe with only informations regarding the images of participants for each cohort. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b1137095-0ac0-42f2-b0d9-3e1399a6a4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fmri_cohort_baseline = df_cohort_baseline[[COL_PAT_ID, COL_VISIT_TYPE, 'Description', \n",
    "                                              'Imaging Protocol', 'Image ID']]\n",
    "\n",
    "df_fmri_cohort_1y = df_cohort_baseline[[COL_PAT_ID, COL_VISIT_TYPE, 'Description', \n",
    "                                              'Imaging Protocol', 'Image ID']][\n",
    "                                        df_cohort_baseline[COL_PAT_ID].isin(df_cohort_1y[COL_PAT_ID].tolist())]\n",
    "\n",
    "df_fmri_cohort_2y = df_cohort_baseline[[COL_PAT_ID, COL_VISIT_TYPE, 'Description', \n",
    "                                              'Imaging Protocol', 'Image ID']][\n",
    "                                        df_cohort_baseline[COL_PAT_ID].isin(df_cohort_2y[COL_PAT_ID].tolist())]\n",
    "\n",
    "df_fmri_cohort_4y = df_cohort_baseline[[COL_PAT_ID, COL_VISIT_TYPE, 'Description', \n",
    "                                              'Imaging Protocol', 'Image ID']][\n",
    "                                        df_cohort_baseline[COL_PAT_ID].isin(df_cohort_4y[COL_PAT_ID].tolist())]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc9c028",
   "metadata": {},
   "source": [
    "fMRI data are associated with anatomical T1 data, often necessary for preprocessing. \n",
    "\n",
    "We searched for the T1 data acquired during the same session as the fMRI ones for the baseline cohort. We chose the first one each time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "03948a95-9536-4bdc-b7e2-c8b4f8c36036",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This notebook was run on 2023-09-07 13:51:23 UTC +0000\n",
      "['COR', 'Coronal', 'Cal Head 24', 'Transverse', 'tra_T1_MPRAGE', 'TRA']\n",
      "['AX', 'Ax', 'axial', 'Phantom', 'T2']\n",
      "{'Screening': 'SC', 'Baseline': 'BL', 'Month 6': 'V02', 'Month 12': 'V04', 'Month 24': 'V06', 'Month 36': 'V08', 'Month 48': 'V10', 'Symptomatic Therapy': 'ST', 'Unscheduled Visit 01': 'U01', 'Unscheduled Visit 02': 'U02', 'Premature Withdrawal': 'PW'}\n",
      "Saved in MRI_info.csv\n"
     ]
    }
   ],
   "source": [
    "from livingpark_utils.scripts import run\n",
    "from livingpark_utils.scripts import mri_metadata\n",
    "\n",
    "run.mri_metadata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a9fd3c40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping 3 subjects with non-integer IDs\n"
     ]
    }
   ],
   "source": [
    "df_mri = load_ppmi_csv(utils, '3D_mri_info.csv', from_ida_search=True)\n",
    "\n",
    "df_mri_cohort_baseline = df_mri.merge(df_cohort_baseline[[COL_PAT_ID, COL_VISIT_TYPE]], \n",
    "                      on = [COL_PAT_ID, COL_VISIT_TYPE])\n",
    "df_mri_cohort_baseline = df_mri_cohort_baseline.sort_values(by=[COL_PAT_ID,'Description'])\n",
    "df_mri_cohort_baseline = df_mri_cohort_baseline.drop_duplicates(subset=[COL_PAT_ID])\n",
    "\n",
    "df_mri_cohort_1y = df_mri_cohort_baseline[[COL_PAT_ID, COL_VISIT_TYPE, 'Description', \n",
    "                                              'Imaging Protocol', 'Image ID']][\n",
    "                                        df_mri_cohort_baseline[COL_PAT_ID].isin(df_cohort_1y[COL_PAT_ID].tolist())]\n",
    "\n",
    "df_mri_cohort_2y = df_mri_cohort_baseline[[COL_PAT_ID, COL_VISIT_TYPE, 'Description', \n",
    "                                              'Imaging Protocol', 'Image ID']][\n",
    "                                        df_mri_cohort_baseline[COL_PAT_ID].isin(df_cohort_2y[COL_PAT_ID].tolist())]\n",
    "\n",
    "df_mri_cohort_4y = df_mri_cohort_baseline[[COL_PAT_ID, COL_VISIT_TYPE, 'Description', \n",
    "                                              'Imaging Protocol', 'Image ID']][\n",
    "                                        df_mri_cohort_baseline[COL_PAT_ID].isin(df_cohort_4y[COL_PAT_ID].tolist())]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16aa6e0-bd3d-4cb4-8491-3137af2f708f",
   "metadata": {},
   "source": [
    "## Download preprocessed images "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ee66d9-9522-483c-b83c-a6fa9cceda97",
   "metadata": {},
   "source": [
    "## Compute imaging features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "6a8a269f-583f-42bd-9eb6-26cbab090436",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nguyenetal.workflows import confound_reg_wf\n",
    "from glob import glob\n",
    "\n",
    "subject_list = [f.split('/')[-1].split('-')[-1] for f in glob('/home/nguyen-etal-2021/inputs/data/Nifti/sub-*')]\n",
    "\n",
    "data_dir = '/home/nguyen-etal-2021/inputs/data/Nifti/derivatives/fmriprep'\n",
    "output_dir = '/home/nguyen-etal-2021/inputs/data/Nifti/derivatives/confounds_filter'\n",
    "confounds_list = ['csf', 'white_matter', 'trans_x', 'trans_y', 'trans_z', 'rot_x', 'rot_y', 'rot_z']\n",
    "\n",
    "func_file_template = join('sub-{subject_id}', 'func', 'sub-{subject_id}_task-rest_space-MNI152NLin2009cAsym_desc-preproc_bold.nii.gz')\n",
    "confounds_file_template= join('sub-{subject_id}', 'func', 'sub-{subject_id}_task-rest_desc-confounds_timeseries.tsv')\n",
    "\n",
    "filtering_wf = confound_reg_wf.NoiseRegression_Pipeline(subject_list, data_dir, output_dir, \n",
    "                                confounds_list, confounds_file_template, func_file_template,\n",
    "                                run_ICA=False)\n",
    "filtering_wf.pipeline.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6a3f501d-60f5-4093-a10a-7b343943f364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "230907-19:18:19,499 nipype.workflow INFO:\n",
      "\t Workflow static_measures_wf settings: ['check', 'execution', 'logging', 'monitoring']\n",
      "230907-19:18:19,614 nipype.workflow INFO:\n",
      "\t Running serially.\n",
      "230907-19:18:19,617 nipype.workflow INFO:\n",
      "\t [Node] Setting-up \"static_measures_wf.select_files\" in \"/home/nguyen-etal-2021/.cache/inputs/data/Nifti/derivatives/static_measures/static_measures_wf/_subject_id_3107/select_files\".\n",
      "230907-19:18:19,678 nipype.workflow INFO:\n",
      "\t [Node] Executing \"select_files\" <nipype.interfaces.io.SelectFiles>\n",
      "230907-19:18:19,688 nipype.workflow INFO:\n",
      "\t [Node] Finished \"select_files\", elapsed time 0.004435s.\n",
      "230907-19:18:19,722 nipype.workflow INFO:\n",
      "\t [Node] Setting-up \"static_measures_wf.alff_workflow.bandpass_filtering\" in \"/home/nguyen-etal-2021/.cache/inputs/data/Nifti/derivatives/static_measures/static_measures_wf/alff_workflow/_subject_id_3107/bandpass_filtering\".\n",
      "230907-19:18:19,749 nipype.workflow INFO:\n",
      "\t [Node] Cached \"static_measures_wf.alff_workflow.bandpass_filtering\" - collecting precomputed outputs\n",
      "230907-19:18:19,751 nipype.workflow INFO:\n",
      "\t [Node] \"static_measures_wf.alff_workflow.bandpass_filtering\" found cached.\n",
      "230907-19:18:19,754 nipype.workflow INFO:\n",
      "\t [Node] Setting-up \"static_measures_wf.alff_workflow.get_option_string\" in \"/home/nguyen-etal-2021/.cache/inputs/data/Nifti/derivatives/static_measures/static_measures_wf/alff_workflow/_subject_id_3107/get_option_string\".\n",
      "230907-19:18:19,781 nipype.workflow INFO:\n",
      "\t [Node] Cached \"static_measures_wf.alff_workflow.get_option_string\" - collecting precomputed outputs\n",
      "230907-19:18:19,784 nipype.workflow INFO:\n",
      "\t [Node] \"static_measures_wf.alff_workflow.get_option_string\" found cached.\n",
      "230907-19:18:19,786 nipype.workflow INFO:\n",
      "\t [Node] Setting-up \"static_measures_wf.reho_workflow.reho_map\" in \"/home/nguyen-etal-2021/.cache/inputs/data/Nifti/derivatives/static_measures/static_measures_wf/reho_workflow/_subject_id_3107/reho_map\".\n",
      "230907-19:18:19,817 nipype.workflow INFO:\n",
      "\t [Node] Cached \"static_measures_wf.reho_workflow.reho_map\" - collecting precomputed outputs\n",
      "230907-19:18:19,819 nipype.workflow INFO:\n",
      "\t [Node] \"static_measures_wf.reho_workflow.reho_map\" found cached.\n",
      "230907-19:18:19,821 nipype.workflow INFO:\n",
      "\t [Node] Setting-up \"static_measures_wf.alff_workflow.stddev_filtered\" in \"/home/nguyen-etal-2021/.cache/inputs/data/Nifti/derivatives/static_measures/static_measures_wf/alff_workflow/_subject_id_3107/stddev_filtered\".\n",
      "230907-19:18:19,851 nipype.workflow INFO:\n",
      "\t [Node] Cached \"static_measures_wf.alff_workflow.stddev_filtered\" - collecting precomputed outputs\n",
      "230907-19:18:19,853 nipype.workflow INFO:\n",
      "\t [Node] \"static_measures_wf.alff_workflow.stddev_filtered\" found cached.\n",
      "230907-19:18:19,857 nipype.workflow INFO:\n",
      "\t [Node] Setting-up \"static_measures_wf.alff_workflow.stddev_unfiltered\" in \"/home/nguyen-etal-2021/.cache/inputs/data/Nifti/derivatives/static_measures/static_measures_wf/alff_workflow/_subject_id_3107/stddev_unfiltered\".\n",
      "230907-19:18:19,889 nipype.workflow INFO:\n",
      "\t [Node] Cached \"static_measures_wf.alff_workflow.stddev_unfiltered\" - collecting precomputed outputs\n",
      "230907-19:18:19,892 nipype.workflow INFO:\n",
      "\t [Node] \"static_measures_wf.alff_workflow.stddev_unfiltered\" found cached.\n",
      "230907-19:18:19,896 nipype.workflow INFO:\n",
      "\t [Node] Setting-up \"static_measures_wf.compute_roi_measures_reho\" in \"/home/nguyen-etal-2021/.cache/inputs/data/Nifti/derivatives/static_measures/static_measures_wf/_subject_id_3107/compute_roi_measures_reho\".\n",
      "230907-19:18:19,933 nipype.workflow INFO:\n",
      "\t [Node] Outdated cache found for \"static_measures_wf.compute_roi_measures_reho\".\n",
      "230907-19:18:19,979 nipype.workflow INFO:\n",
      "\t [Node] Executing \"compute_roi_measures_reho\" <nipype.interfaces.utility.wrappers.Function>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda-latest/envs/neuro/lib/python3.10/site-packages/nilearn/datasets/atlas.py:1467: FutureWarning: The default behavior of the function will be deprecated and replaced in release 0.13 to use the new parameters resolution and and version.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "230907-19:18:23,492 nipype.workflow INFO:\n",
      "\t [Node] Finished \"compute_roi_measures_reho\", elapsed time 3.505813s.\n",
      "230907-19:18:23,526 nipype.workflow INFO:\n",
      "\t [Node] Setting-up \"static_measures_wf.alff_workflow.falff\" in \"/home/nguyen-etal-2021/.cache/inputs/data/Nifti/derivatives/static_measures/static_measures_wf/alff_workflow/_subject_id_3107/falff\".\n",
      "230907-19:18:23,559 nipype.workflow INFO:\n",
      "\t [Node] Cached \"static_measures_wf.alff_workflow.falff\" - collecting precomputed outputs\n",
      "230907-19:18:23,562 nipype.workflow INFO:\n",
      "\t [Node] \"static_measures_wf.alff_workflow.falff\" found cached.\n",
      "230907-19:18:23,565 nipype.workflow INFO:\n",
      "\t [Node] Setting-up \"static_measures_wf.datasink\" in \"/home/nguyen-etal-2021/.cache/inputs/data/Nifti/derivatives/static_measures/static_measures_wf/_subject_id_3107/datasink\".\n",
      "230907-19:18:23,645 nipype.workflow INFO:\n",
      "\t [Node] Executing \"datasink\" <nipype.interfaces.io.DataSink>\n",
      "230907-19:18:23,652 nipype.interface INFO:\n",
      "\t sub: /home/nguyen-etal-2021/inputs/data/Nifti/derivatives/static_measures/results/_subject_id_3107/sub-3107_ReHo_atlas-basc197.csv -> /home/nguyen-etal-2021/inputs/data/Nifti/derivatives/static_measures/results/sub-3107/sub-3107_ReHo_atlas-basc197.csv\n",
      "230907-19:18:23,668 nipype.interface INFO:\n",
      "\t sub: /home/nguyen-etal-2021/inputs/data/Nifti/derivatives/static_measures/results/_subject_id_3107/sub-3107_ReHo_atlas-basc444.csv -> /home/nguyen-etal-2021/inputs/data/Nifti/derivatives/static_measures/results/sub-3107/sub-3107_ReHo_atlas-basc444.csv\n",
      "230907-19:18:23,684 nipype.interface INFO:\n",
      "\t sub: /home/nguyen-etal-2021/inputs/data/Nifti/derivatives/static_measures/results/_subject_id_3107/sub-3107_ReHo_atlas-schaefer.csv -> /home/nguyen-etal-2021/inputs/data/Nifti/derivatives/static_measures/results/sub-3107/sub-3107_ReHo_atlas-schaefer.csv\n",
      "230907-19:18:23,700 nipype.interface INFO:\n",
      "\t sub: /home/nguyen-etal-2021/inputs/data/Nifti/derivatives/static_measures/results/_subject_id_3107/residual_filtered_tstat.nii.gz -> /home/nguyen-etal-2021/inputs/data/Nifti/derivatives/static_measures/results/sub-3107/residual_filtered_tstat.nii.gz\n",
      "230907-19:18:23,713 nipype.interface INFO:\n",
      "\t sub: /home/nguyen-etal-2021/inputs/data/Nifti/derivatives/static_measures/results/_subject_id_3107/sub-3107_task-rest_space-MNI152NLin2009cAsym_desc-brain_mask_calc.nii.gz -> /home/nguyen-etal-2021/inputs/data/Nifti/derivatives/static_measures/results/sub-3107/sub-3107_task-rest_space-MNI152NLin2009cAsym_desc-brain_mask_calc.nii.gz\n",
      "230907-19:18:23,721 nipype.interface INFO:\n",
      "\t sub: /home/nguyen-etal-2021/inputs/data/Nifti/derivatives/static_measures/results/_subject_id_3107/ReHo.nii.gz -> /home/nguyen-etal-2021/inputs/data/Nifti/derivatives/static_measures/results/sub-3107/ReHo.nii.gz\n",
      "230907-19:18:23,731 nipype.workflow INFO:\n",
      "\t [Node] Finished \"datasink\", elapsed time 0.080491s.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<networkx.classes.digraph.DiGraph at 0x41e0c32b30>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nguyenetal.workflows import static_measures_wf\n",
    "from glob import glob\n",
    "from os.path import join\n",
    "\n",
    "done = [i.split('/')[-1].split('_')[-1] for i in \\\n",
    "                             glob('/home/nguyen-etal-2021/inputs/data/Nifti/derivatives/static_measures/results/_subject_id_*')]\n",
    "subject_list = [f.split('/')[-1].split('-')[-1] for f in glob('/home/nguyen-etal-2021/inputs/data/Nifti/sub-*') \\\n",
    "                if f.split('/')[-1].split('-')[-1] not in done]\n",
    "\n",
    "data_dir = '/home/nguyen-etal-2021/inputs/data/Nifti/derivatives'\n",
    "output_dir = '/home/nguyen-etal-2021/outputs/static_measures'\n",
    "\n",
    "func_file_template = join('confounds_filter','results','_subject_id_{subject_id}',\n",
    "                          'sub-{subject_id}_task-rest_space-MNI152NLin2009cAsym_desc-preproc_bold_regfilt.nii.gz')\n",
    "mask_file_template = join('fmriprep', 'sub-{subject_id}', 'func', \n",
    "                          'sub-{subject_id}_task-rest_space-MNI152NLin2009cAsym_desc-brain_mask.nii.gz')\n",
    "\n",
    "cereb_atlas = '/home/nguyen-etal-2021/inputs/atlases/Cerebellum-MNIfnirt-maxprob-thr25-2mm.nii.gz'\n",
    "striatum_atlas = '/home/nguyen-etal-2021/inputs/atlases/striatum-con-label-thr25-7sub-2mm.nii.gz'\n",
    "\n",
    "alff_reho_wf = static_measures_wf.StaticMeasures_Pipeline(subject_list, data_dir, output_dir, \n",
    "                                                          mask_file_template, func_file_template,\n",
    "                                                          cereb_atlas, striatum_atlas,\n",
    "                                                          high_pass_filter=0.01, low_pass_filter=0.1, \n",
    "                                                          cluster_size=27)\n",
    "\n",
    "alff_reho_wf.pipeline.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "0974f168-1a97-4b0c-8c09-76d7f8b7d2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn import regions, image, datasets\n",
    "import numpy as np\n",
    "\n",
    "def get_mean_ROI_values(feature_path_list, atlas):\n",
    "    basc_atlas = datasets.fetch_atlas_basc_multiscale_2015()\n",
    "    schaefer_atlas = datasets.fetch_atlas_schaefer_2018(n_rois=100, yeo_networks=7, resolution_mm=2)\n",
    "    cereb_atlas = './inputs/atlases/Cerebellum-MNIfnirt-maxprob-thr25-2mm.nii.gz'\n",
    "    striatum_atlas = './inputs/atlases/striatum-con-label-thr25-7sub-2mm.nii.gz'\n",
    "    \n",
    "    # Paths to brain atlases\n",
    "    atlas_dict = {'basc197': basc_atlas['scale197'],\n",
    "                   'basc444': basc_atlas['scale444'],\n",
    "                   'schaefer': [schaefer_atlas, cereb_atlas, striatum_atlas]\n",
    "                   }\n",
    "    \n",
    "    atlas_path = atlas_dict[atlas]\n",
    "\n",
    "    if atlas == 'schaefer':\n",
    "        output_list = []\n",
    "        atlas_img = image.load_img(atlas_path[0]['maps'])\n",
    "        cereb_atlas_img = image.load_img(atlas_path[1])\n",
    "        striatum_atlas_img = image.load_img(atlas_path[2])\n",
    "        \n",
    "        for i in range(len(feature_path_list)):\n",
    "            atlas_img_res = image.resample_to_img(atlas_img, feature_path_list[i], \n",
    "                                                  interpolation='nearest')\n",
    "            arr_regions, _ = regions.img_to_signals_labels([feature_path_list[i]], atlas_img_res)\n",
    "            \n",
    "            cereb_atlas_img_res = image.resample_to_img(cereb_atlas_img, feature_path_list[i], \n",
    "                                                        interpolation='nearest')\n",
    "            arr_regions_cereb, _ = regions.img_to_signals_labels([feature_path_list[i]], cereb_atlas_img_res)\n",
    "            \n",
    "            striatum_atlas_img_res = image.resample_to_img(striatum_atlas_img, feature_path_list[i], \n",
    "                                                           interpolation='nearest')\n",
    "            arr_regions_striatum, _ = regions.img_to_signals_labels([feature_path_list[i]], striatum_atlas_img_res)\n",
    "    \n",
    "            arr_cort_cereb = np.append(arr_regions[0], arr_regions_cereb[0])\n",
    "            arr_cort_cereb_striatum = np.append(arr_cort_cereb, arr_regions_striatum[0])\n",
    "\n",
    "            output_list.append(arr_cort_cereb_striatum)\n",
    "            \n",
    "    else:\n",
    "        output_list = []\n",
    "        atlas_img = image.load_img(atlas_path)\n",
    "        for i in range(len(feature_path_list)):\n",
    "            atlas_img_res = image.resample_to_img(atlas_img, feature_path_list[i], interpolation='nearest')\n",
    "            arr_regions, _ = regions.img_to_signals_labels([feature_path_list[i]], atlas_img_res)\n",
    "            \n",
    "            output_list.append(arr_regions[0])\n",
    "    \n",
    "    return output_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "5093078f-ede1-4e85-bd84-6784e25820a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ReHo computation with the different atlases\n",
    "# from glob import glob\n",
    "# import pandas as pd\n",
    "# feature_path_list = sorted(glob('./inputs/data/Nifti/derivatives/static_measures/results/_subject_id_*/ReHo.nii.gz'))\n",
    "\n",
    "# for atlas in ['basc197', 'basc444', 'schaefer']:\n",
    "#     df = pd.DataFrame(get_mean_ROI_values(feature_path_list, atlas))\n",
    "#     df.to_csv(f'./inputs/features/reho_{atlas}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "6d183f67-cb11-48a1-991e-32bf247ef576",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fALFF computation with the different atlases\n",
    "feature_path_list = sorted(glob('./inputs/data/Nifti/derivatives/static_measures/results/_subject_id_*/sub-*_task-rest_space-MNI152NLin2009cAsym_desc-brain_mask_calc.nii.gz'))\n",
    "\n",
    "for atlas in ['basc197', 'basc444', 'schaefer']:\n",
    "    df = pd.DataFrame(get_mean_ROI_values(feature_path_list, atlas))\n",
    "    df.to_csv(f'./inputs/features/falff_{atlas}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "236e3bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ALFF computation with the different atlases\n",
    "# feature_path_list = sorted(glob('./inputs/data/Nifti/derivatives/static_measures/results/_subject_id_*/residual_filtered_tstat.nii.gz'))\n",
    "\n",
    "# for atlas in ['basc197', 'basc444', 'schaefer']:\n",
    "#     df = pd.DataFrame(get_mean_ROI_values(feature_path_list, atlas))\n",
    "#     df.to_csv(f'./inputs/features/alff_{atlas}.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db047e0",
   "metadata": {},
   "source": [
    "## Extracting features for machine learning models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36461a9",
   "metadata": {},
   "source": [
    "In the paper, authors reported having used different clinical and demographic features along with the radiomic data to train the models. \n",
    "These included:\n",
    "* **Clinical features**: disease duration, symptom duration, dominant symptom side, Geriatric Depression Scale (GDS), Montreal Cognitive Assessment (MoCA), and presence of tremor, rigidity, or postural instability at baseline. Baseline MDS-UPDRS score was also included as a confounding variable when training models to predict future outcomes. \n",
    "* **Demographic features**: age, sex, ethnicity, race, handedness, and years of education.\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "    Dominant side is not included on PD features in the PPMI database, so we removed it from the used features.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "beb4c137-6315-47f2-87e1-663f7e617096",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features_cohort_baseline = get_features(df_cohort_baseline, utils,\n",
    "                                  df_cohort_baseline[COL_PAT_ID].tolist(), \n",
    "                                 timepoint='baseline')\n",
    "\n",
    "df_features_cohort_1y = get_features(df_cohort_baseline, utils,\n",
    "                                  df_cohort_1y[COL_PAT_ID].tolist(), \n",
    "                                 timepoint='baseline')\n",
    "\n",
    "df_features_cohort_2y = get_features(df_cohort_baseline, utils,\n",
    "                                  df_cohort_2y[COL_PAT_ID].tolist(), \n",
    "                                 timepoint='baseline')\n",
    "\n",
    "df_features_cohort_4y = get_features(df_cohort_baseline, utils,\n",
    "                                  df_cohort_4y[COL_PAT_ID].tolist(), \n",
    "                                 timepoint='baseline')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "58707931-c9e4-4a3b-a335-f797911eed48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>RANOS</th>\n",
       "      <th>HANDED</th>\n",
       "      <th>DXTREMOR</th>\n",
       "      <th>DXRIGID</th>\n",
       "      <th>DXBRADY</th>\n",
       "      <th>DXPOSINS</th>\n",
       "      <th>EDUCYRS</th>\n",
       "      <th>AGE_AT_VISIT</th>\n",
       "      <th>V-DXDT</th>\n",
       "      <th>V-SXDT</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PATNO</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3107.0</th>\n",
       "      <td>0.078906</td>\n",
       "      <td>0.107629</td>\n",
       "      <td>0.112039</td>\n",
       "      <td>0.152554</td>\n",
       "      <td>0.144017</td>\n",
       "      <td>0.141698</td>\n",
       "      <td>0.099577</td>\n",
       "      <td>0.115480</td>\n",
       "      <td>0.166412</td>\n",
       "      <td>0.079800</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>71.7</td>\n",
       "      <td>59.0</td>\n",
       "      <td>1277.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3108.0</th>\n",
       "      <td>0.063657</td>\n",
       "      <td>0.123462</td>\n",
       "      <td>0.141663</td>\n",
       "      <td>0.116689</td>\n",
       "      <td>0.120959</td>\n",
       "      <td>0.167787</td>\n",
       "      <td>0.227511</td>\n",
       "      <td>0.204358</td>\n",
       "      <td>0.216383</td>\n",
       "      <td>0.112243</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>51.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>911.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3113.0</th>\n",
       "      <td>0.130151</td>\n",
       "      <td>0.121835</td>\n",
       "      <td>0.288024</td>\n",
       "      <td>0.101565</td>\n",
       "      <td>0.203078</td>\n",
       "      <td>0.411984</td>\n",
       "      <td>0.127998</td>\n",
       "      <td>0.210179</td>\n",
       "      <td>0.384110</td>\n",
       "      <td>0.122838</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>61.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>730.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3116.0</th>\n",
       "      <td>0.099790</td>\n",
       "      <td>0.077995</td>\n",
       "      <td>0.186360</td>\n",
       "      <td>0.124088</td>\n",
       "      <td>0.141277</td>\n",
       "      <td>0.284977</td>\n",
       "      <td>0.149453</td>\n",
       "      <td>0.135368</td>\n",
       "      <td>0.232303</td>\n",
       "      <td>0.136215</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>669.0</td>\n",
       "      <td>1095.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3118.0</th>\n",
       "      <td>0.101579</td>\n",
       "      <td>0.065066</td>\n",
       "      <td>0.112533</td>\n",
       "      <td>0.077043</td>\n",
       "      <td>0.079815</td>\n",
       "      <td>0.139485</td>\n",
       "      <td>0.080737</td>\n",
       "      <td>0.108440</td>\n",
       "      <td>0.146949</td>\n",
       "      <td>0.102199</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>64.4</td>\n",
       "      <td>31.0</td>\n",
       "      <td>214.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3119.0</th>\n",
       "      <td>0.091444</td>\n",
       "      <td>0.097531</td>\n",
       "      <td>0.103519</td>\n",
       "      <td>0.139209</td>\n",
       "      <td>0.111908</td>\n",
       "      <td>0.083378</td>\n",
       "      <td>0.116150</td>\n",
       "      <td>0.106859</td>\n",
       "      <td>0.133346</td>\n",
       "      <td>0.091286</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>66.4</td>\n",
       "      <td>62.0</td>\n",
       "      <td>761.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3120.0</th>\n",
       "      <td>0.080788</td>\n",
       "      <td>0.223751</td>\n",
       "      <td>0.168203</td>\n",
       "      <td>0.080803</td>\n",
       "      <td>0.097696</td>\n",
       "      <td>0.120485</td>\n",
       "      <td>0.092300</td>\n",
       "      <td>0.091364</td>\n",
       "      <td>0.106202</td>\n",
       "      <td>0.102287</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>53.5</td>\n",
       "      <td>29.0</td>\n",
       "      <td>547.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3123.0</th>\n",
       "      <td>0.058312</td>\n",
       "      <td>0.195478</td>\n",
       "      <td>0.150687</td>\n",
       "      <td>0.142458</td>\n",
       "      <td>0.143466</td>\n",
       "      <td>0.165722</td>\n",
       "      <td>0.119811</td>\n",
       "      <td>0.135140</td>\n",
       "      <td>0.186328</td>\n",
       "      <td>0.117301</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>70.4</td>\n",
       "      <td>61.0</td>\n",
       "      <td>61.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3124.0</th>\n",
       "      <td>0.058802</td>\n",
       "      <td>0.085047</td>\n",
       "      <td>0.066328</td>\n",
       "      <td>0.088517</td>\n",
       "      <td>0.054273</td>\n",
       "      <td>0.066635</td>\n",
       "      <td>0.100186</td>\n",
       "      <td>0.098203</td>\n",
       "      <td>0.074058</td>\n",
       "      <td>0.068365</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>58.3</td>\n",
       "      <td>90.0</td>\n",
       "      <td>274.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3125.0</th>\n",
       "      <td>0.074629</td>\n",
       "      <td>0.051481</td>\n",
       "      <td>0.090919</td>\n",
       "      <td>0.064895</td>\n",
       "      <td>0.127294</td>\n",
       "      <td>0.091070</td>\n",
       "      <td>0.084915</td>\n",
       "      <td>0.087072</td>\n",
       "      <td>0.108789</td>\n",
       "      <td>0.099433</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>121.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3126.0</th>\n",
       "      <td>0.107418</td>\n",
       "      <td>0.165966</td>\n",
       "      <td>0.122592</td>\n",
       "      <td>0.160578</td>\n",
       "      <td>0.126483</td>\n",
       "      <td>0.116398</td>\n",
       "      <td>0.160258</td>\n",
       "      <td>0.114136</td>\n",
       "      <td>0.099447</td>\n",
       "      <td>0.146216</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>65.4</td>\n",
       "      <td>182.0</td>\n",
       "      <td>547.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3127.0</th>\n",
       "      <td>0.083775</td>\n",
       "      <td>0.167580</td>\n",
       "      <td>0.145291</td>\n",
       "      <td>0.126690</td>\n",
       "      <td>0.170123</td>\n",
       "      <td>0.187627</td>\n",
       "      <td>0.222750</td>\n",
       "      <td>0.104040</td>\n",
       "      <td>0.147448</td>\n",
       "      <td>0.097323</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>53.5</td>\n",
       "      <td>92.0</td>\n",
       "      <td>274.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3128.0</th>\n",
       "      <td>0.086018</td>\n",
       "      <td>0.105233</td>\n",
       "      <td>0.135877</td>\n",
       "      <td>0.088883</td>\n",
       "      <td>0.095511</td>\n",
       "      <td>0.094362</td>\n",
       "      <td>0.064980</td>\n",
       "      <td>0.090170</td>\n",
       "      <td>0.083298</td>\n",
       "      <td>0.060727</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>397.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3130.0</th>\n",
       "      <td>0.081472</td>\n",
       "      <td>0.079059</td>\n",
       "      <td>0.089826</td>\n",
       "      <td>0.087520</td>\n",
       "      <td>0.137871</td>\n",
       "      <td>0.081670</td>\n",
       "      <td>0.056829</td>\n",
       "      <td>0.079955</td>\n",
       "      <td>0.118585</td>\n",
       "      <td>0.172958</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>43.9</td>\n",
       "      <td>30.0</td>\n",
       "      <td>274.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3131.0</th>\n",
       "      <td>0.134620</td>\n",
       "      <td>0.105626</td>\n",
       "      <td>0.109698</td>\n",
       "      <td>0.180505</td>\n",
       "      <td>0.133868</td>\n",
       "      <td>0.317453</td>\n",
       "      <td>0.129982</td>\n",
       "      <td>0.150532</td>\n",
       "      <td>0.139716</td>\n",
       "      <td>0.110533</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>72.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>306.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3132.0</th>\n",
       "      <td>0.081243</td>\n",
       "      <td>0.172382</td>\n",
       "      <td>0.198646</td>\n",
       "      <td>0.196773</td>\n",
       "      <td>0.217615</td>\n",
       "      <td>0.169823</td>\n",
       "      <td>0.147624</td>\n",
       "      <td>0.099069</td>\n",
       "      <td>0.145272</td>\n",
       "      <td>0.064081</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>53.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>366.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3134.0</th>\n",
       "      <td>0.151742</td>\n",
       "      <td>0.153891</td>\n",
       "      <td>0.194889</td>\n",
       "      <td>0.078227</td>\n",
       "      <td>0.109212</td>\n",
       "      <td>0.085973</td>\n",
       "      <td>0.074791</td>\n",
       "      <td>0.065152</td>\n",
       "      <td>0.086679</td>\n",
       "      <td>0.105861</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>38.5</td>\n",
       "      <td>59.0</td>\n",
       "      <td>182.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3327.0</th>\n",
       "      <td>0.112351</td>\n",
       "      <td>0.136136</td>\n",
       "      <td>0.266844</td>\n",
       "      <td>0.092145</td>\n",
       "      <td>0.253234</td>\n",
       "      <td>0.375091</td>\n",
       "      <td>0.101834</td>\n",
       "      <td>0.114933</td>\n",
       "      <td>0.167547</td>\n",
       "      <td>0.103603</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>54.4</td>\n",
       "      <td>92.0</td>\n",
       "      <td>366.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3332.0</th>\n",
       "      <td>0.095979</td>\n",
       "      <td>0.106815</td>\n",
       "      <td>0.083572</td>\n",
       "      <td>0.108135</td>\n",
       "      <td>0.074487</td>\n",
       "      <td>0.102005</td>\n",
       "      <td>0.126624</td>\n",
       "      <td>0.097418</td>\n",
       "      <td>0.090474</td>\n",
       "      <td>0.136614</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>74.1</td>\n",
       "      <td>28.0</td>\n",
       "      <td>394.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3352.0</th>\n",
       "      <td>0.061083</td>\n",
       "      <td>0.101252</td>\n",
       "      <td>0.116833</td>\n",
       "      <td>0.064267</td>\n",
       "      <td>0.090382</td>\n",
       "      <td>0.122254</td>\n",
       "      <td>0.068934</td>\n",
       "      <td>0.122277</td>\n",
       "      <td>0.185226</td>\n",
       "      <td>0.087472</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>53.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>669.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3354.0</th>\n",
       "      <td>0.103747</td>\n",
       "      <td>0.170916</td>\n",
       "      <td>0.157899</td>\n",
       "      <td>0.115180</td>\n",
       "      <td>0.060683</td>\n",
       "      <td>0.196965</td>\n",
       "      <td>0.105670</td>\n",
       "      <td>0.116471</td>\n",
       "      <td>0.221233</td>\n",
       "      <td>0.077378</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>71.1</td>\n",
       "      <td>699.0</td>\n",
       "      <td>820.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3359.0</th>\n",
       "      <td>0.122619</td>\n",
       "      <td>0.099342</td>\n",
       "      <td>0.188299</td>\n",
       "      <td>0.095635</td>\n",
       "      <td>0.079998</td>\n",
       "      <td>0.132760</td>\n",
       "      <td>0.126152</td>\n",
       "      <td>0.107733</td>\n",
       "      <td>0.196520</td>\n",
       "      <td>0.110836</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>74.7</td>\n",
       "      <td>28.0</td>\n",
       "      <td>638.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3360.0</th>\n",
       "      <td>0.121516</td>\n",
       "      <td>0.125488</td>\n",
       "      <td>0.210654</td>\n",
       "      <td>0.117818</td>\n",
       "      <td>0.075503</td>\n",
       "      <td>0.105574</td>\n",
       "      <td>0.115409</td>\n",
       "      <td>0.090907</td>\n",
       "      <td>0.126075</td>\n",
       "      <td>0.058932</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>69.3</td>\n",
       "      <td>90.0</td>\n",
       "      <td>121.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3364.0</th>\n",
       "      <td>0.089804</td>\n",
       "      <td>0.073046</td>\n",
       "      <td>0.077190</td>\n",
       "      <td>0.134792</td>\n",
       "      <td>0.136378</td>\n",
       "      <td>0.104188</td>\n",
       "      <td>0.106756</td>\n",
       "      <td>0.102270</td>\n",
       "      <td>0.069110</td>\n",
       "      <td>0.093637</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>41.2</td>\n",
       "      <td>31.0</td>\n",
       "      <td>304.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3365.0</th>\n",
       "      <td>0.118453</td>\n",
       "      <td>0.248244</td>\n",
       "      <td>0.201685</td>\n",
       "      <td>0.134919</td>\n",
       "      <td>0.170304</td>\n",
       "      <td>0.154050</td>\n",
       "      <td>0.128927</td>\n",
       "      <td>0.134355</td>\n",
       "      <td>0.264517</td>\n",
       "      <td>0.112089</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>51.1</td>\n",
       "      <td>30.0</td>\n",
       "      <td>426.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3366.0</th>\n",
       "      <td>0.111513</td>\n",
       "      <td>0.117335</td>\n",
       "      <td>0.145222</td>\n",
       "      <td>0.107769</td>\n",
       "      <td>0.109388</td>\n",
       "      <td>0.126587</td>\n",
       "      <td>0.074191</td>\n",
       "      <td>0.160935</td>\n",
       "      <td>0.217902</td>\n",
       "      <td>0.108161</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>63.4</td>\n",
       "      <td>30.0</td>\n",
       "      <td>181.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3367.0</th>\n",
       "      <td>0.109118</td>\n",
       "      <td>0.149967</td>\n",
       "      <td>0.195197</td>\n",
       "      <td>0.184649</td>\n",
       "      <td>0.216360</td>\n",
       "      <td>0.261316</td>\n",
       "      <td>0.241920</td>\n",
       "      <td>0.180464</td>\n",
       "      <td>0.219178</td>\n",
       "      <td>0.139811</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>47.2</td>\n",
       "      <td>153.0</td>\n",
       "      <td>365.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3371.0</th>\n",
       "      <td>0.064401</td>\n",
       "      <td>0.139499</td>\n",
       "      <td>0.133577</td>\n",
       "      <td>0.077276</td>\n",
       "      <td>0.112803</td>\n",
       "      <td>0.105017</td>\n",
       "      <td>0.093115</td>\n",
       "      <td>0.094606</td>\n",
       "      <td>0.113206</td>\n",
       "      <td>0.121560</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>69.7</td>\n",
       "      <td>699.0</td>\n",
       "      <td>760.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3372.0</th>\n",
       "      <td>0.094447</td>\n",
       "      <td>0.122183</td>\n",
       "      <td>0.087730</td>\n",
       "      <td>0.073838</td>\n",
       "      <td>0.113100</td>\n",
       "      <td>0.103140</td>\n",
       "      <td>0.077003</td>\n",
       "      <td>0.076946</td>\n",
       "      <td>0.063224</td>\n",
       "      <td>0.073983</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>72.4</td>\n",
       "      <td>92.0</td>\n",
       "      <td>1218.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3373.0</th>\n",
       "      <td>0.097151</td>\n",
       "      <td>0.087790</td>\n",
       "      <td>0.108537</td>\n",
       "      <td>0.061142</td>\n",
       "      <td>0.086275</td>\n",
       "      <td>0.112524</td>\n",
       "      <td>0.061610</td>\n",
       "      <td>0.112785</td>\n",
       "      <td>0.092435</td>\n",
       "      <td>0.063249</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>62.1</td>\n",
       "      <td>60.0</td>\n",
       "      <td>274.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3374.0</th>\n",
       "      <td>0.073557</td>\n",
       "      <td>0.272945</td>\n",
       "      <td>0.129134</td>\n",
       "      <td>0.201670</td>\n",
       "      <td>0.239463</td>\n",
       "      <td>0.135827</td>\n",
       "      <td>0.156261</td>\n",
       "      <td>0.243035</td>\n",
       "      <td>0.110505</td>\n",
       "      <td>0.097052</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>72.6</td>\n",
       "      <td>121.0</td>\n",
       "      <td>335.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3375.0</th>\n",
       "      <td>0.107138</td>\n",
       "      <td>0.143735</td>\n",
       "      <td>0.315173</td>\n",
       "      <td>0.125413</td>\n",
       "      <td>0.176795</td>\n",
       "      <td>0.274803</td>\n",
       "      <td>0.193867</td>\n",
       "      <td>0.213812</td>\n",
       "      <td>0.248978</td>\n",
       "      <td>0.130934</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>51.1</td>\n",
       "      <td>366.0</td>\n",
       "      <td>731.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3377.0</th>\n",
       "      <td>0.162458</td>\n",
       "      <td>0.206009</td>\n",
       "      <td>0.246659</td>\n",
       "      <td>0.134882</td>\n",
       "      <td>0.171303</td>\n",
       "      <td>0.402079</td>\n",
       "      <td>0.210273</td>\n",
       "      <td>0.167430</td>\n",
       "      <td>0.267569</td>\n",
       "      <td>0.093440</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>55.8</td>\n",
       "      <td>152.0</td>\n",
       "      <td>701.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3378.0</th>\n",
       "      <td>0.099372</td>\n",
       "      <td>0.146734</td>\n",
       "      <td>0.202060</td>\n",
       "      <td>0.150015</td>\n",
       "      <td>0.160677</td>\n",
       "      <td>0.193986</td>\n",
       "      <td>0.129799</td>\n",
       "      <td>0.149386</td>\n",
       "      <td>0.227280</td>\n",
       "      <td>0.103983</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>1096.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3380.0</th>\n",
       "      <td>0.080697</td>\n",
       "      <td>0.115714</td>\n",
       "      <td>0.120121</td>\n",
       "      <td>0.089015</td>\n",
       "      <td>0.153023</td>\n",
       "      <td>0.163666</td>\n",
       "      <td>0.165199</td>\n",
       "      <td>0.147842</td>\n",
       "      <td>0.161663</td>\n",
       "      <td>0.100429</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>71.1</td>\n",
       "      <td>30.0</td>\n",
       "      <td>547.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3383.0</th>\n",
       "      <td>0.079984</td>\n",
       "      <td>0.113295</td>\n",
       "      <td>0.254528</td>\n",
       "      <td>0.196283</td>\n",
       "      <td>0.113714</td>\n",
       "      <td>0.256410</td>\n",
       "      <td>0.111871</td>\n",
       "      <td>0.137436</td>\n",
       "      <td>0.113388</td>\n",
       "      <td>0.074952</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>48.4</td>\n",
       "      <td>243.0</td>\n",
       "      <td>4110.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3385.0</th>\n",
       "      <td>0.087013</td>\n",
       "      <td>0.106687</td>\n",
       "      <td>0.133278</td>\n",
       "      <td>0.088175</td>\n",
       "      <td>0.132773</td>\n",
       "      <td>0.177038</td>\n",
       "      <td>0.088073</td>\n",
       "      <td>0.131571</td>\n",
       "      <td>0.094962</td>\n",
       "      <td>0.100530</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>51.8</td>\n",
       "      <td>550.0</td>\n",
       "      <td>1614.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3386.0</th>\n",
       "      <td>0.095602</td>\n",
       "      <td>0.117012</td>\n",
       "      <td>0.164395</td>\n",
       "      <td>0.113782</td>\n",
       "      <td>0.099657</td>\n",
       "      <td>0.165816</td>\n",
       "      <td>0.100109</td>\n",
       "      <td>0.102059</td>\n",
       "      <td>0.082954</td>\n",
       "      <td>0.104915</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>71.6</td>\n",
       "      <td>275.0</td>\n",
       "      <td>427.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3387.0</th>\n",
       "      <td>0.076767</td>\n",
       "      <td>0.095533</td>\n",
       "      <td>0.128171</td>\n",
       "      <td>0.111820</td>\n",
       "      <td>0.114936</td>\n",
       "      <td>0.102912</td>\n",
       "      <td>0.081114</td>\n",
       "      <td>0.126581</td>\n",
       "      <td>0.085327</td>\n",
       "      <td>0.068820</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>56.3</td>\n",
       "      <td>122.0</td>\n",
       "      <td>519.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3392.0</th>\n",
       "      <td>0.270822</td>\n",
       "      <td>0.280508</td>\n",
       "      <td>0.267879</td>\n",
       "      <td>0.115333</td>\n",
       "      <td>0.220534</td>\n",
       "      <td>0.254011</td>\n",
       "      <td>0.274299</td>\n",
       "      <td>0.220945</td>\n",
       "      <td>0.328240</td>\n",
       "      <td>0.158692</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>50.5</td>\n",
       "      <td>151.0</td>\n",
       "      <td>304.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3552.0</th>\n",
       "      <td>0.122302</td>\n",
       "      <td>0.162999</td>\n",
       "      <td>0.145077</td>\n",
       "      <td>0.146673</td>\n",
       "      <td>0.137149</td>\n",
       "      <td>0.337273</td>\n",
       "      <td>0.159921</td>\n",
       "      <td>0.146337</td>\n",
       "      <td>0.226056</td>\n",
       "      <td>0.160263</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>61.6</td>\n",
       "      <td>123.0</td>\n",
       "      <td>214.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3557.0</th>\n",
       "      <td>0.108007</td>\n",
       "      <td>0.085675</td>\n",
       "      <td>0.183349</td>\n",
       "      <td>0.087449</td>\n",
       "      <td>0.075558</td>\n",
       "      <td>0.180951</td>\n",
       "      <td>0.118646</td>\n",
       "      <td>0.121741</td>\n",
       "      <td>0.152156</td>\n",
       "      <td>0.098125</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>63.2</td>\n",
       "      <td>699.0</td>\n",
       "      <td>1065.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3567.0</th>\n",
       "      <td>0.104249</td>\n",
       "      <td>0.121719</td>\n",
       "      <td>0.119195</td>\n",
       "      <td>0.108874</td>\n",
       "      <td>0.090517</td>\n",
       "      <td>0.137309</td>\n",
       "      <td>0.101202</td>\n",
       "      <td>0.110235</td>\n",
       "      <td>0.116116</td>\n",
       "      <td>0.111795</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>69.8</td>\n",
       "      <td>59.0</td>\n",
       "      <td>304.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3574.0</th>\n",
       "      <td>0.103010</td>\n",
       "      <td>0.120970</td>\n",
       "      <td>0.192070</td>\n",
       "      <td>0.113569</td>\n",
       "      <td>0.121540</td>\n",
       "      <td>0.178421</td>\n",
       "      <td>0.131852</td>\n",
       "      <td>0.136873</td>\n",
       "      <td>0.160947</td>\n",
       "      <td>0.081357</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>67.2</td>\n",
       "      <td>184.0</td>\n",
       "      <td>365.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3575.0</th>\n",
       "      <td>0.070299</td>\n",
       "      <td>0.118897</td>\n",
       "      <td>0.135990</td>\n",
       "      <td>0.085477</td>\n",
       "      <td>0.074782</td>\n",
       "      <td>0.152226</td>\n",
       "      <td>0.071817</td>\n",
       "      <td>0.075482</td>\n",
       "      <td>0.102903</td>\n",
       "      <td>0.098699</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>62.1</td>\n",
       "      <td>62.0</td>\n",
       "      <td>304.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3577.0</th>\n",
       "      <td>0.077922</td>\n",
       "      <td>0.128818</td>\n",
       "      <td>0.099900</td>\n",
       "      <td>0.070245</td>\n",
       "      <td>0.141512</td>\n",
       "      <td>0.201095</td>\n",
       "      <td>0.078932</td>\n",
       "      <td>0.103322</td>\n",
       "      <td>0.177405</td>\n",
       "      <td>0.069805</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>71.8</td>\n",
       "      <td>31.0</td>\n",
       "      <td>973.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3585.0</th>\n",
       "      <td>0.077350</td>\n",
       "      <td>0.096934</td>\n",
       "      <td>0.094555</td>\n",
       "      <td>0.102098</td>\n",
       "      <td>0.116065</td>\n",
       "      <td>0.075483</td>\n",
       "      <td>0.118111</td>\n",
       "      <td>0.108418</td>\n",
       "      <td>0.089710</td>\n",
       "      <td>0.059644</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>53.4</td>\n",
       "      <td>701.0</td>\n",
       "      <td>1004.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3586.0</th>\n",
       "      <td>0.089386</td>\n",
       "      <td>0.120348</td>\n",
       "      <td>0.126190</td>\n",
       "      <td>0.148182</td>\n",
       "      <td>0.133079</td>\n",
       "      <td>0.109989</td>\n",
       "      <td>0.153203</td>\n",
       "      <td>0.118269</td>\n",
       "      <td>0.114674</td>\n",
       "      <td>0.089000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>336.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3587.0</th>\n",
       "      <td>0.059843</td>\n",
       "      <td>0.088077</td>\n",
       "      <td>0.086920</td>\n",
       "      <td>0.082736</td>\n",
       "      <td>0.109188</td>\n",
       "      <td>0.097067</td>\n",
       "      <td>0.115238</td>\n",
       "      <td>0.102783</td>\n",
       "      <td>0.123155</td>\n",
       "      <td>0.100162</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>54.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>366.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3588.0</th>\n",
       "      <td>0.209848</td>\n",
       "      <td>0.124476</td>\n",
       "      <td>0.174271</td>\n",
       "      <td>0.189864</td>\n",
       "      <td>0.195387</td>\n",
       "      <td>0.376096</td>\n",
       "      <td>0.194687</td>\n",
       "      <td>0.150722</td>\n",
       "      <td>0.144563</td>\n",
       "      <td>0.084920</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>53.1</td>\n",
       "      <td>853.0</td>\n",
       "      <td>1919.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3589.0</th>\n",
       "      <td>0.100458</td>\n",
       "      <td>0.125005</td>\n",
       "      <td>0.113611</td>\n",
       "      <td>0.077518</td>\n",
       "      <td>0.092067</td>\n",
       "      <td>0.123027</td>\n",
       "      <td>0.132987</td>\n",
       "      <td>0.133062</td>\n",
       "      <td>0.130560</td>\n",
       "      <td>0.081674</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>153.0</td>\n",
       "      <td>823.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3591.0</th>\n",
       "      <td>0.067694</td>\n",
       "      <td>0.139926</td>\n",
       "      <td>0.103866</td>\n",
       "      <td>0.056000</td>\n",
       "      <td>0.094419</td>\n",
       "      <td>0.109526</td>\n",
       "      <td>0.104712</td>\n",
       "      <td>0.095460</td>\n",
       "      <td>0.148309</td>\n",
       "      <td>0.103058</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>62.7</td>\n",
       "      <td>31.0</td>\n",
       "      <td>458.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3592.0</th>\n",
       "      <td>0.047409</td>\n",
       "      <td>0.085338</td>\n",
       "      <td>0.064016</td>\n",
       "      <td>0.076808</td>\n",
       "      <td>0.113188</td>\n",
       "      <td>0.135516</td>\n",
       "      <td>0.093286</td>\n",
       "      <td>0.123203</td>\n",
       "      <td>0.092731</td>\n",
       "      <td>0.065251</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>62.3</td>\n",
       "      <td>90.0</td>\n",
       "      <td>425.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3593.0</th>\n",
       "      <td>0.124846</td>\n",
       "      <td>0.147672</td>\n",
       "      <td>0.078910</td>\n",
       "      <td>0.107963</td>\n",
       "      <td>0.104335</td>\n",
       "      <td>0.095501</td>\n",
       "      <td>0.131076</td>\n",
       "      <td>0.156903</td>\n",
       "      <td>0.083655</td>\n",
       "      <td>0.085666</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>55.4</td>\n",
       "      <td>28.0</td>\n",
       "      <td>181.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3760.0</th>\n",
       "      <td>0.091661</td>\n",
       "      <td>0.135895</td>\n",
       "      <td>0.122567</td>\n",
       "      <td>0.104930</td>\n",
       "      <td>0.124348</td>\n",
       "      <td>0.173449</td>\n",
       "      <td>0.252573</td>\n",
       "      <td>0.151970</td>\n",
       "      <td>0.249846</td>\n",
       "      <td>0.087781</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>69.5</td>\n",
       "      <td>92.0</td>\n",
       "      <td>607.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3800.0</th>\n",
       "      <td>0.088351</td>\n",
       "      <td>0.074476</td>\n",
       "      <td>0.085638</td>\n",
       "      <td>0.104242</td>\n",
       "      <td>0.076959</td>\n",
       "      <td>0.088745</td>\n",
       "      <td>0.060352</td>\n",
       "      <td>0.062021</td>\n",
       "      <td>0.082239</td>\n",
       "      <td>0.060685</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>42.8</td>\n",
       "      <td>90.0</td>\n",
       "      <td>1551.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3802.0</th>\n",
       "      <td>0.110868</td>\n",
       "      <td>0.108046</td>\n",
       "      <td>0.105298</td>\n",
       "      <td>0.089785</td>\n",
       "      <td>0.106480</td>\n",
       "      <td>0.072226</td>\n",
       "      <td>0.070069</td>\n",
       "      <td>0.097970</td>\n",
       "      <td>0.083251</td>\n",
       "      <td>0.091705</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>89.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3808.0</th>\n",
       "      <td>0.050576</td>\n",
       "      <td>0.124392</td>\n",
       "      <td>0.090203</td>\n",
       "      <td>0.091298</td>\n",
       "      <td>0.096847</td>\n",
       "      <td>0.088058</td>\n",
       "      <td>0.070117</td>\n",
       "      <td>0.129657</td>\n",
       "      <td>0.099559</td>\n",
       "      <td>0.082787</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>59.2</td>\n",
       "      <td>426.0</td>\n",
       "      <td>791.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3814.0</th>\n",
       "      <td>0.094816</td>\n",
       "      <td>0.119796</td>\n",
       "      <td>0.091700</td>\n",
       "      <td>0.153676</td>\n",
       "      <td>0.176185</td>\n",
       "      <td>0.161125</td>\n",
       "      <td>0.112510</td>\n",
       "      <td>0.089447</td>\n",
       "      <td>0.105340</td>\n",
       "      <td>0.066980</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>69.5</td>\n",
       "      <td>30.0</td>\n",
       "      <td>183.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3815.0</th>\n",
       "      <td>0.115475</td>\n",
       "      <td>0.175265</td>\n",
       "      <td>0.265757</td>\n",
       "      <td>0.176454</td>\n",
       "      <td>0.114733</td>\n",
       "      <td>0.149633</td>\n",
       "      <td>0.095709</td>\n",
       "      <td>0.105179</td>\n",
       "      <td>0.174612</td>\n",
       "      <td>0.066251</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>64.4</td>\n",
       "      <td>61.0</td>\n",
       "      <td>273.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3818.0</th>\n",
       "      <td>0.165246</td>\n",
       "      <td>0.176408</td>\n",
       "      <td>0.201556</td>\n",
       "      <td>0.155798</td>\n",
       "      <td>0.158429</td>\n",
       "      <td>0.098041</td>\n",
       "      <td>0.138455</td>\n",
       "      <td>0.120738</td>\n",
       "      <td>0.120344</td>\n",
       "      <td>0.099350</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>74.8</td>\n",
       "      <td>29.0</td>\n",
       "      <td>305.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3819.0</th>\n",
       "      <td>0.090775</td>\n",
       "      <td>0.111305</td>\n",
       "      <td>0.130141</td>\n",
       "      <td>0.123203</td>\n",
       "      <td>0.079711</td>\n",
       "      <td>0.155778</td>\n",
       "      <td>0.088636</td>\n",
       "      <td>0.089083</td>\n",
       "      <td>0.126501</td>\n",
       "      <td>0.071150</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>53.9</td>\n",
       "      <td>60.0</td>\n",
       "      <td>731.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3822.0</th>\n",
       "      <td>0.071767</td>\n",
       "      <td>0.096591</td>\n",
       "      <td>0.093140</td>\n",
       "      <td>0.074034</td>\n",
       "      <td>0.104277</td>\n",
       "      <td>0.069659</td>\n",
       "      <td>0.157362</td>\n",
       "      <td>0.104536</td>\n",
       "      <td>0.082846</td>\n",
       "      <td>0.141186</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>851.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3823.0</th>\n",
       "      <td>0.148491</td>\n",
       "      <td>0.178806</td>\n",
       "      <td>0.180994</td>\n",
       "      <td>0.122521</td>\n",
       "      <td>0.142464</td>\n",
       "      <td>0.201304</td>\n",
       "      <td>0.202024</td>\n",
       "      <td>0.186484</td>\n",
       "      <td>0.105065</td>\n",
       "      <td>0.064922</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>274.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3824.0</th>\n",
       "      <td>0.108413</td>\n",
       "      <td>0.150273</td>\n",
       "      <td>0.136493</td>\n",
       "      <td>0.132118</td>\n",
       "      <td>0.153890</td>\n",
       "      <td>0.129379</td>\n",
       "      <td>0.137936</td>\n",
       "      <td>0.218841</td>\n",
       "      <td>0.178898</td>\n",
       "      <td>0.106311</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>57.3</td>\n",
       "      <td>31.0</td>\n",
       "      <td>244.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3825.0</th>\n",
       "      <td>0.112846</td>\n",
       "      <td>0.106292</td>\n",
       "      <td>0.128571</td>\n",
       "      <td>0.094719</td>\n",
       "      <td>0.114587</td>\n",
       "      <td>0.180461</td>\n",
       "      <td>0.130579</td>\n",
       "      <td>0.086551</td>\n",
       "      <td>0.100337</td>\n",
       "      <td>0.097053</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>59.2</td>\n",
       "      <td>61.0</td>\n",
       "      <td>548.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3826.0</th>\n",
       "      <td>0.116125</td>\n",
       "      <td>0.128841</td>\n",
       "      <td>0.134203</td>\n",
       "      <td>0.099596</td>\n",
       "      <td>0.077969</td>\n",
       "      <td>0.165415</td>\n",
       "      <td>0.126593</td>\n",
       "      <td>0.092491</td>\n",
       "      <td>0.137025</td>\n",
       "      <td>0.107359</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>78.2</td>\n",
       "      <td>92.0</td>\n",
       "      <td>182.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3828.0</th>\n",
       "      <td>0.074399</td>\n",
       "      <td>0.110616</td>\n",
       "      <td>0.142885</td>\n",
       "      <td>0.096744</td>\n",
       "      <td>0.111039</td>\n",
       "      <td>0.116130</td>\n",
       "      <td>0.106698</td>\n",
       "      <td>0.101938</td>\n",
       "      <td>0.090275</td>\n",
       "      <td>0.090756</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>78.3</td>\n",
       "      <td>61.0</td>\n",
       "      <td>244.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3829.0</th>\n",
       "      <td>0.057592</td>\n",
       "      <td>0.096379</td>\n",
       "      <td>0.115884</td>\n",
       "      <td>0.067425</td>\n",
       "      <td>0.069396</td>\n",
       "      <td>0.108286</td>\n",
       "      <td>0.091511</td>\n",
       "      <td>0.106041</td>\n",
       "      <td>0.111046</td>\n",
       "      <td>0.119017</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>68.4</td>\n",
       "      <td>31.0</td>\n",
       "      <td>184.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3830.0</th>\n",
       "      <td>0.098771</td>\n",
       "      <td>0.110716</td>\n",
       "      <td>0.107137</td>\n",
       "      <td>0.113208</td>\n",
       "      <td>0.118317</td>\n",
       "      <td>0.131714</td>\n",
       "      <td>0.089786</td>\n",
       "      <td>0.132898</td>\n",
       "      <td>0.122189</td>\n",
       "      <td>0.134971</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>52.5</td>\n",
       "      <td>92.0</td>\n",
       "      <td>397.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3831.0</th>\n",
       "      <td>0.097567</td>\n",
       "      <td>0.087319</td>\n",
       "      <td>0.178651</td>\n",
       "      <td>0.123624</td>\n",
       "      <td>0.105023</td>\n",
       "      <td>0.196211</td>\n",
       "      <td>0.122764</td>\n",
       "      <td>0.098593</td>\n",
       "      <td>0.115376</td>\n",
       "      <td>0.093646</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>76.4</td>\n",
       "      <td>123.0</td>\n",
       "      <td>884.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3832.0</th>\n",
       "      <td>0.118839</td>\n",
       "      <td>0.177190</td>\n",
       "      <td>0.170949</td>\n",
       "      <td>0.095177</td>\n",
       "      <td>0.094099</td>\n",
       "      <td>0.131919</td>\n",
       "      <td>0.178591</td>\n",
       "      <td>0.130040</td>\n",
       "      <td>0.188654</td>\n",
       "      <td>0.110954</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>65.5</td>\n",
       "      <td>31.0</td>\n",
       "      <td>305.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3834.0</th>\n",
       "      <td>0.112591</td>\n",
       "      <td>0.106199</td>\n",
       "      <td>0.270106</td>\n",
       "      <td>0.132734</td>\n",
       "      <td>0.134155</td>\n",
       "      <td>0.282787</td>\n",
       "      <td>0.111079</td>\n",
       "      <td>0.105778</td>\n",
       "      <td>0.225851</td>\n",
       "      <td>0.094336</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>69.7</td>\n",
       "      <td>183.0</td>\n",
       "      <td>304.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3835.0</th>\n",
       "      <td>0.135876</td>\n",
       "      <td>0.106157</td>\n",
       "      <td>0.224977</td>\n",
       "      <td>0.094399</td>\n",
       "      <td>0.182689</td>\n",
       "      <td>0.141326</td>\n",
       "      <td>0.084125</td>\n",
       "      <td>0.100499</td>\n",
       "      <td>0.220474</td>\n",
       "      <td>0.073699</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>73.7</td>\n",
       "      <td>61.0</td>\n",
       "      <td>92.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3838.0</th>\n",
       "      <td>0.069905</td>\n",
       "      <td>0.121851</td>\n",
       "      <td>0.241824</td>\n",
       "      <td>0.089609</td>\n",
       "      <td>0.101522</td>\n",
       "      <td>0.302172</td>\n",
       "      <td>0.090438</td>\n",
       "      <td>0.100875</td>\n",
       "      <td>0.118138</td>\n",
       "      <td>0.087718</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>61.9</td>\n",
       "      <td>59.0</td>\n",
       "      <td>334.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3870.0</th>\n",
       "      <td>0.102029</td>\n",
       "      <td>0.226488</td>\n",
       "      <td>0.256707</td>\n",
       "      <td>0.173833</td>\n",
       "      <td>0.138998</td>\n",
       "      <td>0.186524</td>\n",
       "      <td>0.083817</td>\n",
       "      <td>0.184961</td>\n",
       "      <td>0.185820</td>\n",
       "      <td>0.104651</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>41.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>123.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4011.0</th>\n",
       "      <td>0.176248</td>\n",
       "      <td>0.137978</td>\n",
       "      <td>0.151769</td>\n",
       "      <td>0.144490</td>\n",
       "      <td>0.078699</td>\n",
       "      <td>0.131658</td>\n",
       "      <td>0.122265</td>\n",
       "      <td>0.216909</td>\n",
       "      <td>0.136648</td>\n",
       "      <td>0.108350</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>55.6</td>\n",
       "      <td>822.0</td>\n",
       "      <td>973.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4019.0</th>\n",
       "      <td>0.091575</td>\n",
       "      <td>0.146611</td>\n",
       "      <td>0.095463</td>\n",
       "      <td>0.163036</td>\n",
       "      <td>0.134077</td>\n",
       "      <td>0.093603</td>\n",
       "      <td>0.143523</td>\n",
       "      <td>0.156827</td>\n",
       "      <td>0.082176</td>\n",
       "      <td>0.073480</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>58.8</td>\n",
       "      <td>31.0</td>\n",
       "      <td>1826.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4020.0</th>\n",
       "      <td>0.093670</td>\n",
       "      <td>0.326406</td>\n",
       "      <td>0.234850</td>\n",
       "      <td>0.129379</td>\n",
       "      <td>0.244519</td>\n",
       "      <td>0.366991</td>\n",
       "      <td>0.193899</td>\n",
       "      <td>0.204123</td>\n",
       "      <td>0.263913</td>\n",
       "      <td>0.129364</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>64.8</td>\n",
       "      <td>31.0</td>\n",
       "      <td>1095.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4021.0</th>\n",
       "      <td>0.088006</td>\n",
       "      <td>0.106804</td>\n",
       "      <td>0.147582</td>\n",
       "      <td>0.111346</td>\n",
       "      <td>0.066653</td>\n",
       "      <td>0.139669</td>\n",
       "      <td>0.099007</td>\n",
       "      <td>0.178887</td>\n",
       "      <td>0.166359</td>\n",
       "      <td>0.075132</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>76.9</td>\n",
       "      <td>761.0</td>\n",
       "      <td>7336.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4022.0</th>\n",
       "      <td>0.091803</td>\n",
       "      <td>0.151280</td>\n",
       "      <td>0.133825</td>\n",
       "      <td>0.186187</td>\n",
       "      <td>0.096161</td>\n",
       "      <td>0.060561</td>\n",
       "      <td>0.097757</td>\n",
       "      <td>0.149532</td>\n",
       "      <td>0.180985</td>\n",
       "      <td>0.090251</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>51.5</td>\n",
       "      <td>153.0</td>\n",
       "      <td>276.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4024.0</th>\n",
       "      <td>0.078272</td>\n",
       "      <td>0.069562</td>\n",
       "      <td>0.083214</td>\n",
       "      <td>0.178341</td>\n",
       "      <td>0.131190</td>\n",
       "      <td>0.078633</td>\n",
       "      <td>0.128927</td>\n",
       "      <td>0.124567</td>\n",
       "      <td>0.097933</td>\n",
       "      <td>0.086702</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>76.3</td>\n",
       "      <td>61.0</td>\n",
       "      <td>517.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4026.0</th>\n",
       "      <td>0.093833</td>\n",
       "      <td>0.110541</td>\n",
       "      <td>0.113897</td>\n",
       "      <td>0.187328</td>\n",
       "      <td>0.138571</td>\n",
       "      <td>0.129222</td>\n",
       "      <td>0.135448</td>\n",
       "      <td>0.117466</td>\n",
       "      <td>0.152504</td>\n",
       "      <td>0.092910</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>304.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4029.0</th>\n",
       "      <td>0.107428</td>\n",
       "      <td>0.166896</td>\n",
       "      <td>0.117484</td>\n",
       "      <td>0.189414</td>\n",
       "      <td>0.129774</td>\n",
       "      <td>0.107405</td>\n",
       "      <td>0.089731</td>\n",
       "      <td>0.122328</td>\n",
       "      <td>0.214082</td>\n",
       "      <td>0.070158</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>52.8</td>\n",
       "      <td>62.0</td>\n",
       "      <td>397.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4030.0</th>\n",
       "      <td>0.099834</td>\n",
       "      <td>0.072230</td>\n",
       "      <td>0.114612</td>\n",
       "      <td>0.083767</td>\n",
       "      <td>0.098845</td>\n",
       "      <td>0.160008</td>\n",
       "      <td>0.116527</td>\n",
       "      <td>0.099718</td>\n",
       "      <td>0.068698</td>\n",
       "      <td>0.121906</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>74.7</td>\n",
       "      <td>611.0</td>\n",
       "      <td>1614.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4034.0</th>\n",
       "      <td>0.128786</td>\n",
       "      <td>0.101379</td>\n",
       "      <td>0.133926</td>\n",
       "      <td>0.086103</td>\n",
       "      <td>0.099998</td>\n",
       "      <td>0.084968</td>\n",
       "      <td>0.107746</td>\n",
       "      <td>0.097870</td>\n",
       "      <td>0.094302</td>\n",
       "      <td>0.083816</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>884.0</td>\n",
       "      <td>1827.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4035.0</th>\n",
       "      <td>0.080373</td>\n",
       "      <td>0.119736</td>\n",
       "      <td>0.117742</td>\n",
       "      <td>0.157333</td>\n",
       "      <td>0.226711</td>\n",
       "      <td>0.121733</td>\n",
       "      <td>0.138297</td>\n",
       "      <td>0.136299</td>\n",
       "      <td>0.137384</td>\n",
       "      <td>0.078614</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>60.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>153.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4037.0</th>\n",
       "      <td>0.063912</td>\n",
       "      <td>0.117823</td>\n",
       "      <td>0.130463</td>\n",
       "      <td>0.121254</td>\n",
       "      <td>0.110566</td>\n",
       "      <td>0.078172</td>\n",
       "      <td>0.144371</td>\n",
       "      <td>0.103990</td>\n",
       "      <td>0.072665</td>\n",
       "      <td>0.069434</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>52.8</td>\n",
       "      <td>184.0</td>\n",
       "      <td>550.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4038.0</th>\n",
       "      <td>0.097656</td>\n",
       "      <td>0.097008</td>\n",
       "      <td>0.138211</td>\n",
       "      <td>0.115966</td>\n",
       "      <td>0.178887</td>\n",
       "      <td>0.184214</td>\n",
       "      <td>0.123763</td>\n",
       "      <td>0.116620</td>\n",
       "      <td>0.157896</td>\n",
       "      <td>0.102436</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>70.8</td>\n",
       "      <td>28.0</td>\n",
       "      <td>304.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40338.0</th>\n",
       "      <td>0.131936</td>\n",
       "      <td>0.106626</td>\n",
       "      <td>0.165186</td>\n",
       "      <td>0.077751</td>\n",
       "      <td>0.079635</td>\n",
       "      <td>0.106133</td>\n",
       "      <td>0.092818</td>\n",
       "      <td>0.113855</td>\n",
       "      <td>0.139544</td>\n",
       "      <td>0.104713</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>68.9</td>\n",
       "      <td>730.0</td>\n",
       "      <td>761.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40366.0</th>\n",
       "      <td>0.079517</td>\n",
       "      <td>0.112213</td>\n",
       "      <td>0.209140</td>\n",
       "      <td>0.138111</td>\n",
       "      <td>0.153930</td>\n",
       "      <td>0.117227</td>\n",
       "      <td>0.090590</td>\n",
       "      <td>0.106686</td>\n",
       "      <td>0.107988</td>\n",
       "      <td>0.065698</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>1642.0</td>\n",
       "      <td>2007.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40533.0</th>\n",
       "      <td>0.073889</td>\n",
       "      <td>0.130580</td>\n",
       "      <td>0.145862</td>\n",
       "      <td>0.123098</td>\n",
       "      <td>0.113117</td>\n",
       "      <td>0.149915</td>\n",
       "      <td>0.151132</td>\n",
       "      <td>0.160210</td>\n",
       "      <td>0.202085</td>\n",
       "      <td>0.182451</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>60.1</td>\n",
       "      <td>486.0</td>\n",
       "      <td>639.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40534.0</th>\n",
       "      <td>0.209264</td>\n",
       "      <td>0.124369</td>\n",
       "      <td>0.243704</td>\n",
       "      <td>0.142040</td>\n",
       "      <td>0.226450</td>\n",
       "      <td>0.428408</td>\n",
       "      <td>0.099976</td>\n",
       "      <td>0.129411</td>\n",
       "      <td>0.193906</td>\n",
       "      <td>0.108638</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>67.1</td>\n",
       "      <td>1369.0</td>\n",
       "      <td>2527.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50485.0</th>\n",
       "      <td>0.118235</td>\n",
       "      <td>0.158058</td>\n",
       "      <td>0.183614</td>\n",
       "      <td>0.158130</td>\n",
       "      <td>0.316001</td>\n",
       "      <td>0.163348</td>\n",
       "      <td>0.195473</td>\n",
       "      <td>0.229053</td>\n",
       "      <td>0.113639</td>\n",
       "      <td>0.208518</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>63.3</td>\n",
       "      <td>2405.0</td>\n",
       "      <td>2587.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50901.0</th>\n",
       "      <td>0.057190</td>\n",
       "      <td>0.088560</td>\n",
       "      <td>0.101027</td>\n",
       "      <td>0.066164</td>\n",
       "      <td>0.132044</td>\n",
       "      <td>0.120599</td>\n",
       "      <td>0.055493</td>\n",
       "      <td>0.117077</td>\n",
       "      <td>0.177640</td>\n",
       "      <td>0.089686</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>54.3</td>\n",
       "      <td>153.0</td>\n",
       "      <td>549.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51632.0</th>\n",
       "      <td>0.068065</td>\n",
       "      <td>0.105421</td>\n",
       "      <td>0.088146</td>\n",
       "      <td>0.189117</td>\n",
       "      <td>0.235626</td>\n",
       "      <td>0.131288</td>\n",
       "      <td>0.173010</td>\n",
       "      <td>0.153928</td>\n",
       "      <td>0.191823</td>\n",
       "      <td>0.107944</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>62.9</td>\n",
       "      <td>1492.0</td>\n",
       "      <td>2283.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51731.0</th>\n",
       "      <td>0.101082</td>\n",
       "      <td>0.147924</td>\n",
       "      <td>0.109880</td>\n",
       "      <td>0.110856</td>\n",
       "      <td>0.158286</td>\n",
       "      <td>0.146952</td>\n",
       "      <td>0.089022</td>\n",
       "      <td>0.121419</td>\n",
       "      <td>0.094521</td>\n",
       "      <td>0.146314</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>70.8</td>\n",
       "      <td>911.0</td>\n",
       "      <td>1734.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52678.0</th>\n",
       "      <td>0.090151</td>\n",
       "      <td>0.089917</td>\n",
       "      <td>0.142260</td>\n",
       "      <td>0.117413</td>\n",
       "      <td>0.062914</td>\n",
       "      <td>0.112473</td>\n",
       "      <td>0.120355</td>\n",
       "      <td>0.214917</td>\n",
       "      <td>0.132561</td>\n",
       "      <td>0.101714</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>68.7</td>\n",
       "      <td>273.0</td>\n",
       "      <td>1522.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53060.0</th>\n",
       "      <td>0.103092</td>\n",
       "      <td>0.171809</td>\n",
       "      <td>0.205385</td>\n",
       "      <td>0.204191</td>\n",
       "      <td>0.168243</td>\n",
       "      <td>0.264304</td>\n",
       "      <td>0.203898</td>\n",
       "      <td>0.314057</td>\n",
       "      <td>0.310437</td>\n",
       "      <td>0.078212</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>68.1</td>\n",
       "      <td>854.0</td>\n",
       "      <td>1188.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55395.0</th>\n",
       "      <td>0.081994</td>\n",
       "      <td>0.110795</td>\n",
       "      <td>0.135574</td>\n",
       "      <td>0.135466</td>\n",
       "      <td>0.279012</td>\n",
       "      <td>0.137960</td>\n",
       "      <td>0.139770</td>\n",
       "      <td>0.122819</td>\n",
       "      <td>0.139120</td>\n",
       "      <td>0.100969</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>66.9</td>\n",
       "      <td>1340.0</td>\n",
       "      <td>1522.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55468.0</th>\n",
       "      <td>0.148710</td>\n",
       "      <td>0.078821</td>\n",
       "      <td>0.174450</td>\n",
       "      <td>0.096104</td>\n",
       "      <td>0.188373</td>\n",
       "      <td>0.209109</td>\n",
       "      <td>0.118928</td>\n",
       "      <td>0.151052</td>\n",
       "      <td>0.246344</td>\n",
       "      <td>0.111706</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>51.6</td>\n",
       "      <td>1492.0</td>\n",
       "      <td>1857.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70463.0</th>\n",
       "      <td>0.071867</td>\n",
       "      <td>0.088942</td>\n",
       "      <td>0.111905</td>\n",
       "      <td>0.143909</td>\n",
       "      <td>0.099220</td>\n",
       "      <td>0.140540</td>\n",
       "      <td>0.174451</td>\n",
       "      <td>0.164741</td>\n",
       "      <td>0.109926</td>\n",
       "      <td>0.095079</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>1127.0</td>\n",
       "      <td>1765.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>102 rows × 154 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                0         1         2         3         4         5         6  \\\n",
       "PATNO                                                                           \n",
       "3107.0   0.078906  0.107629  0.112039  0.152554  0.144017  0.141698  0.099577   \n",
       "3108.0   0.063657  0.123462  0.141663  0.116689  0.120959  0.167787  0.227511   \n",
       "3113.0   0.130151  0.121835  0.288024  0.101565  0.203078  0.411984  0.127998   \n",
       "3116.0   0.099790  0.077995  0.186360  0.124088  0.141277  0.284977  0.149453   \n",
       "3118.0   0.101579  0.065066  0.112533  0.077043  0.079815  0.139485  0.080737   \n",
       "3119.0   0.091444  0.097531  0.103519  0.139209  0.111908  0.083378  0.116150   \n",
       "3120.0   0.080788  0.223751  0.168203  0.080803  0.097696  0.120485  0.092300   \n",
       "3123.0   0.058312  0.195478  0.150687  0.142458  0.143466  0.165722  0.119811   \n",
       "3124.0   0.058802  0.085047  0.066328  0.088517  0.054273  0.066635  0.100186   \n",
       "3125.0   0.074629  0.051481  0.090919  0.064895  0.127294  0.091070  0.084915   \n",
       "3126.0   0.107418  0.165966  0.122592  0.160578  0.126483  0.116398  0.160258   \n",
       "3127.0   0.083775  0.167580  0.145291  0.126690  0.170123  0.187627  0.222750   \n",
       "3128.0   0.086018  0.105233  0.135877  0.088883  0.095511  0.094362  0.064980   \n",
       "3130.0   0.081472  0.079059  0.089826  0.087520  0.137871  0.081670  0.056829   \n",
       "3131.0   0.134620  0.105626  0.109698  0.180505  0.133868  0.317453  0.129982   \n",
       "3132.0   0.081243  0.172382  0.198646  0.196773  0.217615  0.169823  0.147624   \n",
       "3134.0   0.151742  0.153891  0.194889  0.078227  0.109212  0.085973  0.074791   \n",
       "3327.0   0.112351  0.136136  0.266844  0.092145  0.253234  0.375091  0.101834   \n",
       "3332.0   0.095979  0.106815  0.083572  0.108135  0.074487  0.102005  0.126624   \n",
       "3352.0   0.061083  0.101252  0.116833  0.064267  0.090382  0.122254  0.068934   \n",
       "3354.0   0.103747  0.170916  0.157899  0.115180  0.060683  0.196965  0.105670   \n",
       "3359.0   0.122619  0.099342  0.188299  0.095635  0.079998  0.132760  0.126152   \n",
       "3360.0   0.121516  0.125488  0.210654  0.117818  0.075503  0.105574  0.115409   \n",
       "3364.0   0.089804  0.073046  0.077190  0.134792  0.136378  0.104188  0.106756   \n",
       "3365.0   0.118453  0.248244  0.201685  0.134919  0.170304  0.154050  0.128927   \n",
       "3366.0   0.111513  0.117335  0.145222  0.107769  0.109388  0.126587  0.074191   \n",
       "3367.0   0.109118  0.149967  0.195197  0.184649  0.216360  0.261316  0.241920   \n",
       "3371.0   0.064401  0.139499  0.133577  0.077276  0.112803  0.105017  0.093115   \n",
       "3372.0   0.094447  0.122183  0.087730  0.073838  0.113100  0.103140  0.077003   \n",
       "3373.0   0.097151  0.087790  0.108537  0.061142  0.086275  0.112524  0.061610   \n",
       "3374.0   0.073557  0.272945  0.129134  0.201670  0.239463  0.135827  0.156261   \n",
       "3375.0   0.107138  0.143735  0.315173  0.125413  0.176795  0.274803  0.193867   \n",
       "3377.0   0.162458  0.206009  0.246659  0.134882  0.171303  0.402079  0.210273   \n",
       "3378.0   0.099372  0.146734  0.202060  0.150015  0.160677  0.193986  0.129799   \n",
       "3380.0   0.080697  0.115714  0.120121  0.089015  0.153023  0.163666  0.165199   \n",
       "3383.0   0.079984  0.113295  0.254528  0.196283  0.113714  0.256410  0.111871   \n",
       "3385.0   0.087013  0.106687  0.133278  0.088175  0.132773  0.177038  0.088073   \n",
       "3386.0   0.095602  0.117012  0.164395  0.113782  0.099657  0.165816  0.100109   \n",
       "3387.0   0.076767  0.095533  0.128171  0.111820  0.114936  0.102912  0.081114   \n",
       "3392.0   0.270822  0.280508  0.267879  0.115333  0.220534  0.254011  0.274299   \n",
       "3552.0   0.122302  0.162999  0.145077  0.146673  0.137149  0.337273  0.159921   \n",
       "3557.0   0.108007  0.085675  0.183349  0.087449  0.075558  0.180951  0.118646   \n",
       "3567.0   0.104249  0.121719  0.119195  0.108874  0.090517  0.137309  0.101202   \n",
       "3574.0   0.103010  0.120970  0.192070  0.113569  0.121540  0.178421  0.131852   \n",
       "3575.0   0.070299  0.118897  0.135990  0.085477  0.074782  0.152226  0.071817   \n",
       "3577.0   0.077922  0.128818  0.099900  0.070245  0.141512  0.201095  0.078932   \n",
       "3585.0   0.077350  0.096934  0.094555  0.102098  0.116065  0.075483  0.118111   \n",
       "3586.0   0.089386  0.120348  0.126190  0.148182  0.133079  0.109989  0.153203   \n",
       "3587.0   0.059843  0.088077  0.086920  0.082736  0.109188  0.097067  0.115238   \n",
       "3588.0   0.209848  0.124476  0.174271  0.189864  0.195387  0.376096  0.194687   \n",
       "3589.0   0.100458  0.125005  0.113611  0.077518  0.092067  0.123027  0.132987   \n",
       "3591.0   0.067694  0.139926  0.103866  0.056000  0.094419  0.109526  0.104712   \n",
       "3592.0   0.047409  0.085338  0.064016  0.076808  0.113188  0.135516  0.093286   \n",
       "3593.0   0.124846  0.147672  0.078910  0.107963  0.104335  0.095501  0.131076   \n",
       "3760.0   0.091661  0.135895  0.122567  0.104930  0.124348  0.173449  0.252573   \n",
       "3800.0   0.088351  0.074476  0.085638  0.104242  0.076959  0.088745  0.060352   \n",
       "3802.0   0.110868  0.108046  0.105298  0.089785  0.106480  0.072226  0.070069   \n",
       "3808.0   0.050576  0.124392  0.090203  0.091298  0.096847  0.088058  0.070117   \n",
       "3814.0   0.094816  0.119796  0.091700  0.153676  0.176185  0.161125  0.112510   \n",
       "3815.0   0.115475  0.175265  0.265757  0.176454  0.114733  0.149633  0.095709   \n",
       "3818.0   0.165246  0.176408  0.201556  0.155798  0.158429  0.098041  0.138455   \n",
       "3819.0   0.090775  0.111305  0.130141  0.123203  0.079711  0.155778  0.088636   \n",
       "3822.0   0.071767  0.096591  0.093140  0.074034  0.104277  0.069659  0.157362   \n",
       "3823.0   0.148491  0.178806  0.180994  0.122521  0.142464  0.201304  0.202024   \n",
       "3824.0   0.108413  0.150273  0.136493  0.132118  0.153890  0.129379  0.137936   \n",
       "3825.0   0.112846  0.106292  0.128571  0.094719  0.114587  0.180461  0.130579   \n",
       "3826.0   0.116125  0.128841  0.134203  0.099596  0.077969  0.165415  0.126593   \n",
       "3828.0   0.074399  0.110616  0.142885  0.096744  0.111039  0.116130  0.106698   \n",
       "3829.0   0.057592  0.096379  0.115884  0.067425  0.069396  0.108286  0.091511   \n",
       "3830.0   0.098771  0.110716  0.107137  0.113208  0.118317  0.131714  0.089786   \n",
       "3831.0   0.097567  0.087319  0.178651  0.123624  0.105023  0.196211  0.122764   \n",
       "3832.0   0.118839  0.177190  0.170949  0.095177  0.094099  0.131919  0.178591   \n",
       "3834.0   0.112591  0.106199  0.270106  0.132734  0.134155  0.282787  0.111079   \n",
       "3835.0   0.135876  0.106157  0.224977  0.094399  0.182689  0.141326  0.084125   \n",
       "3838.0   0.069905  0.121851  0.241824  0.089609  0.101522  0.302172  0.090438   \n",
       "3870.0   0.102029  0.226488  0.256707  0.173833  0.138998  0.186524  0.083817   \n",
       "4011.0   0.176248  0.137978  0.151769  0.144490  0.078699  0.131658  0.122265   \n",
       "4019.0   0.091575  0.146611  0.095463  0.163036  0.134077  0.093603  0.143523   \n",
       "4020.0   0.093670  0.326406  0.234850  0.129379  0.244519  0.366991  0.193899   \n",
       "4021.0   0.088006  0.106804  0.147582  0.111346  0.066653  0.139669  0.099007   \n",
       "4022.0   0.091803  0.151280  0.133825  0.186187  0.096161  0.060561  0.097757   \n",
       "4024.0   0.078272  0.069562  0.083214  0.178341  0.131190  0.078633  0.128927   \n",
       "4026.0   0.093833  0.110541  0.113897  0.187328  0.138571  0.129222  0.135448   \n",
       "4029.0   0.107428  0.166896  0.117484  0.189414  0.129774  0.107405  0.089731   \n",
       "4030.0   0.099834  0.072230  0.114612  0.083767  0.098845  0.160008  0.116527   \n",
       "4034.0   0.128786  0.101379  0.133926  0.086103  0.099998  0.084968  0.107746   \n",
       "4035.0   0.080373  0.119736  0.117742  0.157333  0.226711  0.121733  0.138297   \n",
       "4037.0   0.063912  0.117823  0.130463  0.121254  0.110566  0.078172  0.144371   \n",
       "4038.0   0.097656  0.097008  0.138211  0.115966  0.178887  0.184214  0.123763   \n",
       "40338.0  0.131936  0.106626  0.165186  0.077751  0.079635  0.106133  0.092818   \n",
       "40366.0  0.079517  0.112213  0.209140  0.138111  0.153930  0.117227  0.090590   \n",
       "40533.0  0.073889  0.130580  0.145862  0.123098  0.113117  0.149915  0.151132   \n",
       "40534.0  0.209264  0.124369  0.243704  0.142040  0.226450  0.428408  0.099976   \n",
       "50485.0  0.118235  0.158058  0.183614  0.158130  0.316001  0.163348  0.195473   \n",
       "50901.0  0.057190  0.088560  0.101027  0.066164  0.132044  0.120599  0.055493   \n",
       "51632.0  0.068065  0.105421  0.088146  0.189117  0.235626  0.131288  0.173010   \n",
       "51731.0  0.101082  0.147924  0.109880  0.110856  0.158286  0.146952  0.089022   \n",
       "52678.0  0.090151  0.089917  0.142260  0.117413  0.062914  0.112473  0.120355   \n",
       "53060.0  0.103092  0.171809  0.205385  0.204191  0.168243  0.264304  0.203898   \n",
       "55395.0  0.081994  0.110795  0.135574  0.135466  0.279012  0.137960  0.139770   \n",
       "55468.0  0.148710  0.078821  0.174450  0.096104  0.188373  0.209109  0.118928   \n",
       "70463.0  0.071867  0.088942  0.111905  0.143909  0.099220  0.140540  0.174451   \n",
       "\n",
       "                7         8         9  ...  RANOS  HANDED  DXTREMOR  DXRIGID  \\\n",
       "PATNO                                  ...                                     \n",
       "3107.0   0.115480  0.166412  0.079800  ...    0.0     1.0       1.0      1.0   \n",
       "3108.0   0.204358  0.216383  0.112243  ...    0.0     1.0       1.0      1.0   \n",
       "3113.0   0.210179  0.384110  0.122838  ...    0.0     1.0       1.0      1.0   \n",
       "3116.0   0.135368  0.232303  0.136215  ...    0.0     2.0       1.0      1.0   \n",
       "3118.0   0.108440  0.146949  0.102199  ...    0.0     1.0       1.0      1.0   \n",
       "3119.0   0.106859  0.133346  0.091286  ...    0.0     1.0       1.0      1.0   \n",
       "3120.0   0.091364  0.106202  0.102287  ...    0.0     1.0       1.0      1.0   \n",
       "3123.0   0.135140  0.186328  0.117301  ...    0.0     1.0       1.0      1.0   \n",
       "3124.0   0.098203  0.074058  0.068365  ...    0.0     3.0       1.0      1.0   \n",
       "3125.0   0.087072  0.108789  0.099433  ...    0.0     2.0       1.0      1.0   \n",
       "3126.0   0.114136  0.099447  0.146216  ...    0.0     1.0       1.0      1.0   \n",
       "3127.0   0.104040  0.147448  0.097323  ...    0.0     1.0       1.0      1.0   \n",
       "3128.0   0.090170  0.083298  0.060727  ...    0.0     1.0       1.0      1.0   \n",
       "3130.0   0.079955  0.118585  0.172958  ...    0.0     1.0       1.0      1.0   \n",
       "3131.0   0.150532  0.139716  0.110533  ...    0.0     1.0       1.0      1.0   \n",
       "3132.0   0.099069  0.145272  0.064081  ...    0.0     1.0       1.0      1.0   \n",
       "3134.0   0.065152  0.086679  0.105861  ...    0.0     1.0       1.0      1.0   \n",
       "3327.0   0.114933  0.167547  0.103603  ...    0.0     1.0       0.0      1.0   \n",
       "3332.0   0.097418  0.090474  0.136614  ...    0.0     1.0       1.0      1.0   \n",
       "3352.0   0.122277  0.185226  0.087472  ...    0.0     1.0       1.0      1.0   \n",
       "3354.0   0.116471  0.221233  0.077378  ...    0.0     1.0       1.0      2.0   \n",
       "3359.0   0.107733  0.196520  0.110836  ...    0.0     1.0       1.0      1.0   \n",
       "3360.0   0.090907  0.126075  0.058932  ...    0.0     1.0       1.0      1.0   \n",
       "3364.0   0.102270  0.069110  0.093637  ...    0.0     1.0       1.0      1.0   \n",
       "3365.0   0.134355  0.264517  0.112089  ...    0.0     1.0       1.0      0.0   \n",
       "3366.0   0.160935  0.217902  0.108161  ...    0.0     1.0       0.0      1.0   \n",
       "3367.0   0.180464  0.219178  0.139811  ...    0.0     1.0       1.0      1.0   \n",
       "3371.0   0.094606  0.113206  0.121560  ...    0.0     1.0       1.0      1.0   \n",
       "3372.0   0.076946  0.063224  0.073983  ...    0.0     2.0       1.0      1.0   \n",
       "3373.0   0.112785  0.092435  0.063249  ...    0.0     1.0       1.0      0.0   \n",
       "3374.0   0.243035  0.110505  0.097052  ...    0.0     1.0       1.0      1.0   \n",
       "3375.0   0.213812  0.248978  0.130934  ...    0.0     1.0       1.0      0.0   \n",
       "3377.0   0.167430  0.267569  0.093440  ...    0.0     2.0       1.0      0.0   \n",
       "3378.0   0.149386  0.227280  0.103983  ...    0.0     1.0       0.0      1.0   \n",
       "3380.0   0.147842  0.161663  0.100429  ...    0.0     1.0       1.0      1.0   \n",
       "3383.0   0.137436  0.113388  0.074952  ...    0.0     1.0       1.0      1.0   \n",
       "3385.0   0.131571  0.094962  0.100530  ...    0.0     1.0       1.0      1.0   \n",
       "3386.0   0.102059  0.082954  0.104915  ...    0.0     1.0       1.0      1.0   \n",
       "3387.0   0.126581  0.085327  0.068820  ...    0.0     1.0       0.0      1.0   \n",
       "3392.0   0.220945  0.328240  0.158692  ...    0.0     1.0       1.0      0.0   \n",
       "3552.0   0.146337  0.226056  0.160263  ...    0.0     1.0       1.0      1.0   \n",
       "3557.0   0.121741  0.152156  0.098125  ...    0.0     1.0       1.0      1.0   \n",
       "3567.0   0.110235  0.116116  0.111795  ...    0.0     1.0       1.0      0.0   \n",
       "3574.0   0.136873  0.160947  0.081357  ...    0.0     1.0       1.0      1.0   \n",
       "3575.0   0.075482  0.102903  0.098699  ...    0.0     1.0       1.0      1.0   \n",
       "3577.0   0.103322  0.177405  0.069805  ...    0.0     1.0       1.0      1.0   \n",
       "3585.0   0.108418  0.089710  0.059644  ...    0.0     1.0       1.0      1.0   \n",
       "3586.0   0.118269  0.114674  0.089000  ...    0.0     1.0       1.0      1.0   \n",
       "3587.0   0.102783  0.123155  0.100162  ...    0.0     1.0       0.0      1.0   \n",
       "3588.0   0.150722  0.144563  0.084920  ...    0.0     1.0       1.0      1.0   \n",
       "3589.0   0.133062  0.130560  0.081674  ...    0.0     1.0       0.0      1.0   \n",
       "3591.0   0.095460  0.148309  0.103058  ...    0.0     1.0       1.0      1.0   \n",
       "3592.0   0.123203  0.092731  0.065251  ...    0.0     2.0       0.0      1.0   \n",
       "3593.0   0.156903  0.083655  0.085666  ...    0.0     1.0       1.0      1.0   \n",
       "3760.0   0.151970  0.249846  0.087781  ...    0.0     1.0       0.0      1.0   \n",
       "3800.0   0.062021  0.082239  0.060685  ...    0.0     1.0       0.0      1.0   \n",
       "3802.0   0.097970  0.083251  0.091705  ...    0.0     1.0       0.0      1.0   \n",
       "3808.0   0.129657  0.099559  0.082787  ...    0.0     1.0       1.0      1.0   \n",
       "3814.0   0.089447  0.105340  0.066980  ...    0.0     1.0       0.0      1.0   \n",
       "3815.0   0.105179  0.174612  0.066251  ...    0.0     1.0       1.0      1.0   \n",
       "3818.0   0.120738  0.120344  0.099350  ...    0.0     1.0       0.0      1.0   \n",
       "3819.0   0.089083  0.126501  0.071150  ...    0.0     1.0       0.0      1.0   \n",
       "3822.0   0.104536  0.082846  0.141186  ...    0.0     1.0       1.0      1.0   \n",
       "3823.0   0.186484  0.105065  0.064922  ...    0.0     1.0       1.0      1.0   \n",
       "3824.0   0.218841  0.178898  0.106311  ...    0.0     1.0       1.0      1.0   \n",
       "3825.0   0.086551  0.100337  0.097053  ...    0.0     1.0       1.0      1.0   \n",
       "3826.0   0.092491  0.137025  0.107359  ...    0.0     1.0       1.0      1.0   \n",
       "3828.0   0.101938  0.090275  0.090756  ...    0.0     1.0       1.0      1.0   \n",
       "3829.0   0.106041  0.111046  0.119017  ...    0.0     1.0       1.0      1.0   \n",
       "3830.0   0.132898  0.122189  0.134971  ...    0.0     1.0       1.0      1.0   \n",
       "3831.0   0.098593  0.115376  0.093646  ...    0.0     1.0       1.0      1.0   \n",
       "3832.0   0.130040  0.188654  0.110954  ...    0.0     1.0       0.0      1.0   \n",
       "3834.0   0.105778  0.225851  0.094336  ...    0.0     1.0       0.0      1.0   \n",
       "3835.0   0.100499  0.220474  0.073699  ...    0.0     1.0       0.0      1.0   \n",
       "3838.0   0.100875  0.118138  0.087718  ...    0.0     1.0       0.0      1.0   \n",
       "3870.0   0.184961  0.185820  0.104651  ...    0.0     2.0       0.0      0.0   \n",
       "4011.0   0.216909  0.136648  0.108350  ...    0.0     2.0       1.0      1.0   \n",
       "4019.0   0.156827  0.082176  0.073480  ...    0.0     1.0       1.0      0.0   \n",
       "4020.0   0.204123  0.263913  0.129364  ...    0.0     3.0       1.0      1.0   \n",
       "4021.0   0.178887  0.166359  0.075132  ...    0.0     1.0       1.0      1.0   \n",
       "4022.0   0.149532  0.180985  0.090251  ...    0.0     1.0       1.0      1.0   \n",
       "4024.0   0.124567  0.097933  0.086702  ...    0.0     1.0       1.0      1.0   \n",
       "4026.0   0.117466  0.152504  0.092910  ...    0.0     3.0       1.0      1.0   \n",
       "4029.0   0.122328  0.214082  0.070158  ...    0.0     1.0       1.0      1.0   \n",
       "4030.0   0.099718  0.068698  0.121906  ...    0.0     1.0       1.0      1.0   \n",
       "4034.0   0.097870  0.094302  0.083816  ...    0.0     1.0       0.0      1.0   \n",
       "4035.0   0.136299  0.137384  0.078614  ...    0.0     1.0       1.0      1.0   \n",
       "4037.0   0.103990  0.072665  0.069434  ...    0.0     1.0       0.0      1.0   \n",
       "4038.0   0.116620  0.157896  0.102436  ...    0.0     1.0       1.0      1.0   \n",
       "40338.0  0.113855  0.139544  0.104713  ...    0.0     1.0       1.0      1.0   \n",
       "40366.0  0.106686  0.107988  0.065698  ...    0.0     1.0       1.0      2.0   \n",
       "40533.0  0.160210  0.202085  0.182451  ...    0.0     1.0       0.0      1.0   \n",
       "40534.0  0.129411  0.193906  0.108638  ...    0.0     1.0       1.0      1.0   \n",
       "50485.0  0.229053  0.113639  0.208518  ...    0.0     2.0       1.0      1.0   \n",
       "50901.0  0.117077  0.177640  0.089686  ...    0.0     1.0       1.0      1.0   \n",
       "51632.0  0.153928  0.191823  0.107944  ...    0.0     1.0       1.0      1.0   \n",
       "51731.0  0.121419  0.094521  0.146314  ...    0.0     1.0       1.0      0.0   \n",
       "52678.0  0.214917  0.132561  0.101714  ...    0.0     1.0       1.0      1.0   \n",
       "53060.0  0.314057  0.310437  0.078212  ...    0.0     1.0       1.0      1.0   \n",
       "55395.0  0.122819  0.139120  0.100969  ...    0.0     1.0       1.0      1.0   \n",
       "55468.0  0.151052  0.246344  0.111706  ...    0.0     1.0       1.0      0.0   \n",
       "70463.0  0.164741  0.109926  0.095079  ...    0.0     1.0       1.0      0.0   \n",
       "\n",
       "         DXBRADY  DXPOSINS  EDUCYRS  AGE_AT_VISIT  V-DXDT  V-SXDT  \n",
       "PATNO                                                              \n",
       "3107.0       1.0       0.0     16.0          71.7    59.0  1277.0  \n",
       "3108.0       1.0       0.0     15.0          51.8     0.0   911.0  \n",
       "3113.0       1.0       0.0     16.0          61.3     0.0   730.0  \n",
       "3116.0       1.0       1.0     18.0          66.0   669.0  1095.0  \n",
       "3118.0       1.0       0.0     14.0          64.4    31.0   214.0  \n",
       "3119.0       1.0       0.0     16.0          66.4    62.0   761.0  \n",
       "3120.0       1.0       0.0     18.0          53.5    29.0   547.0  \n",
       "3123.0       1.0       0.0     18.0          70.4    61.0    61.0  \n",
       "3124.0       1.0       0.0     16.0          58.3    90.0   274.0  \n",
       "3125.0       1.0       0.0     16.0          48.0    31.0   121.0  \n",
       "3126.0       1.0       0.0     18.0          65.4   182.0   547.0  \n",
       "3127.0       1.0       0.0     16.0          53.5    92.0   274.0  \n",
       "3128.0       1.0       0.0     18.0          61.0    31.0   397.0  \n",
       "3130.0       1.0       0.0     18.0          43.9    30.0   274.0  \n",
       "3131.0       1.0       0.0     18.0          72.2     0.0   306.0  \n",
       "3132.0       1.0       0.0     16.0          53.9     0.0   366.0  \n",
       "3134.0       1.0       0.0     17.0          38.5    59.0   182.0  \n",
       "3327.0       1.0       0.0     18.0          54.4    92.0   366.0  \n",
       "3332.0       1.0       0.0     16.0          74.1    28.0   394.0  \n",
       "3352.0       1.0       0.0     12.0          53.8     0.0   669.0  \n",
       "3354.0       1.0       1.0     16.0          71.1   699.0   820.0  \n",
       "3359.0       1.0       0.0     16.0          74.7    28.0   638.0  \n",
       "3360.0       1.0       0.0     16.0          69.3    90.0   121.0  \n",
       "3364.0       1.0       0.0     15.0          41.2    31.0   304.0  \n",
       "3365.0       0.0       0.0     16.0          51.1    30.0   426.0  \n",
       "3366.0       1.0       0.0     12.0          63.4    30.0   181.0  \n",
       "3367.0       1.0       0.0     16.0          47.2   153.0   365.0  \n",
       "3371.0       1.0       0.0     16.0          69.7   699.0   760.0  \n",
       "3372.0       1.0       0.0     16.0          72.4    92.0  1218.0  \n",
       "3373.0       1.0       0.0     16.0          62.1    60.0   274.0  \n",
       "3374.0       1.0       0.0     16.0          72.6   121.0   335.0  \n",
       "3375.0       1.0       0.0     16.0          51.1   366.0   731.0  \n",
       "3377.0       0.0       0.0     16.0          55.8   152.0   701.0  \n",
       "3378.0       1.0       0.0     20.0          49.0   121.0  1096.0  \n",
       "3380.0       1.0       0.0     17.0          71.1    30.0   547.0  \n",
       "3383.0       1.0       2.0     22.0          48.4   243.0  4110.0  \n",
       "3385.0       2.0       1.0     13.0          51.8   550.0  1614.0  \n",
       "3386.0       1.0       0.0     15.0          71.6   275.0   427.0  \n",
       "3387.0       1.0       0.0     16.0          56.3   122.0   519.0  \n",
       "3392.0       1.0       0.0     19.0          50.5   151.0   304.0  \n",
       "3552.0       1.0       0.0     14.0          61.6   123.0   214.0  \n",
       "3557.0       1.0       1.0     16.0          63.2   699.0  1065.0  \n",
       "3567.0       1.0       0.0     15.0          69.8    59.0   304.0  \n",
       "3574.0       1.0       0.0     18.0          67.2   184.0   365.0  \n",
       "3575.0       1.0       0.0     20.0          62.1    62.0   304.0  \n",
       "3577.0       1.0       0.0     16.0          71.8    31.0   973.0  \n",
       "3585.0       1.0       0.0     14.0          53.4   701.0  1004.0  \n",
       "3586.0       1.0       0.0     16.0          65.0    61.0   336.0  \n",
       "3587.0       1.0       0.0     16.0          54.6     0.0   366.0  \n",
       "3588.0       1.0       0.0     12.0          53.1   853.0  1919.0  \n",
       "3589.0       1.0       0.0     20.0          75.0   153.0   823.0  \n",
       "3591.0       1.0       0.0     17.0          62.7    31.0   458.0  \n",
       "3592.0       1.0       0.0     20.0          62.3    90.0   425.0  \n",
       "3593.0       1.0       0.0     13.0          55.4    28.0   181.0  \n",
       "3760.0       1.0       0.0     18.0          69.5    92.0   607.0  \n",
       "3800.0       1.0       0.0     14.0          42.8    90.0  1551.0  \n",
       "3802.0       1.0       0.0     12.0          74.0    61.0    89.0  \n",
       "3808.0       1.0       2.0     13.0          59.2   426.0   791.0  \n",
       "3814.0       1.0       0.0     15.0          69.5    30.0   183.0  \n",
       "3815.0       1.0       0.0     12.0          64.4    61.0   273.0  \n",
       "3818.0       1.0       0.0     10.0          74.8    29.0   305.0  \n",
       "3819.0       1.0       0.0     10.0          53.9    60.0   731.0  \n",
       "3822.0       1.0       0.0      9.0          57.0    61.0   851.0  \n",
       "3823.0       1.0       0.0     10.0          58.0    30.0   274.0  \n",
       "3824.0       1.0       0.0     11.0          57.3    31.0   244.0  \n",
       "3825.0       1.0       0.0     10.0          59.2    61.0   548.0  \n",
       "3826.0       1.0       0.0     15.0          78.2    92.0   182.0  \n",
       "3828.0       1.0       0.0     14.0          78.3    61.0   244.0  \n",
       "3829.0       1.0       0.0      8.0          68.4    31.0   184.0  \n",
       "3830.0       1.0       0.0     13.0          52.5    92.0   397.0  \n",
       "3831.0       1.0       0.0     14.0          76.4   123.0   884.0  \n",
       "3832.0       1.0       0.0     11.0          65.5    31.0   305.0  \n",
       "3834.0       1.0       0.0     11.0          69.7   183.0   304.0  \n",
       "3835.0       1.0       0.0     10.0          73.7    61.0    92.0  \n",
       "3838.0       1.0       0.0     15.0          61.9    59.0   334.0  \n",
       "3870.0       1.0       0.0     20.0          41.5     0.0   123.0  \n",
       "4011.0       1.0       0.0     16.0          55.6   822.0   973.0  \n",
       "4019.0       1.0       0.0     20.0          58.8    31.0  1826.0  \n",
       "4020.0       1.0       0.0     18.0          64.8    31.0  1095.0  \n",
       "4021.0       1.0       0.0     14.0          76.9   761.0  7336.0  \n",
       "4022.0       1.0       0.0     18.0          51.5   153.0   276.0  \n",
       "4024.0       1.0       0.0     18.0          76.3    61.0   517.0  \n",
       "4026.0       1.0       0.0     18.0          79.0     0.0   304.0  \n",
       "4029.0       1.0       0.0     16.0          52.8    62.0   397.0  \n",
       "4030.0       1.0       0.0     18.0          74.7   611.0  1614.0  \n",
       "4034.0       1.0       0.0     16.0          55.0   884.0  1827.0  \n",
       "4035.0       1.0       0.0     16.0          60.5     0.0   153.0  \n",
       "4037.0       1.0       0.0     12.0          52.8   184.0   550.0  \n",
       "4038.0       1.0       0.0     16.0          70.8    28.0   304.0  \n",
       "40338.0      1.0       1.0     16.0          68.9   730.0   761.0  \n",
       "40366.0      1.0       0.0     16.0          65.0  1642.0  2007.0  \n",
       "40533.0      1.0       0.0     13.0          60.1   486.0   639.0  \n",
       "40534.0      1.0       0.0     18.0          67.1  1369.0  2527.0  \n",
       "50485.0      1.0       0.0     18.0          63.3  2405.0  2587.0  \n",
       "50901.0      1.0       0.0     16.0          54.3   153.0   549.0  \n",
       "51632.0      1.0       1.0     16.0          62.9  1492.0  2283.0  \n",
       "51731.0      0.0       0.0     19.0          70.8   911.0  1734.0  \n",
       "52678.0      1.0       0.0     19.0          68.7   273.0  1522.0  \n",
       "53060.0      1.0       0.0     16.0          68.1   854.0  1188.0  \n",
       "55395.0      1.0       0.0     23.0          66.9  1340.0  1522.0  \n",
       "55468.0      0.0       0.0     13.0          51.6  1492.0  1857.0  \n",
       "70463.0      1.0       0.0     16.0          67.0  1127.0  1765.0  \n",
       "\n",
       "[102 rows x 154 columns]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_reho_cohort_baseline = pd.read_csv('./inputs/features/reho_schaefer.csv', header=0, index_col=0)\n",
    "df_reho_cohort_baseline.index = df_features_cohort_baseline.index\n",
    "df_all_features_cohort_baseline = pd.merge(df_reho_cohort_baseline, df_features_cohort_baseline, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55af4342",
   "metadata": {},
   "source": [
    "## Extracting outcome measures "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e54376-ef52-459f-bcca-57b452abab80",
   "metadata": {},
   "source": [
    "* The outcome of interest is the MDS-UPDRS total score, encompassing both motor and non-motor symptomatology. \n",
    "* As a secondary outcome measure, an MDS-UPDRS score threshold was used to dichotomize subjects into high- and low-severity groups. A threshold of 35 was selected, which was the average of the median MDS-UPDRS score at each of the four timepoints.\n",
    "  \n",
    "It is mentioned in the paper that there were no differences between the high- and low-severity groups in motor predominance (Part III score as a percentage of total score) at p = 0.05.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "7ba81509-2c5e-4f9b-a6d7-3868cae96221",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = get_threshold(df_cohort_baseline, df_cohort_1y, df_cohort_2y, df_cohort_4y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "8e831fff-15b8-41c4-b19c-0f991b51b7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_outcome_baseline = get_outcome_measures(df_cohort_baseline, threshold)\n",
    "df_outcome_1y = get_outcome_measures(df_cohort_1y, threshold)\n",
    "df_outcome_2y = get_outcome_measures(df_cohort_2y, threshold)\n",
    "df_outcome_4y = get_outcome_measures(df_cohort_4y, threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "60e1d45f-8b19-46cd-9290-e711387aea1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline : 0.128\n",
      "2 years : 0.072\n",
      "3 years : 0.643\n",
      "4 years : 0.534\n"
     ]
    }
   ],
   "source": [
    "# Verify that there were no differences between the high- and low-severity groups in motor predominance \n",
    "# (Part III score as a percentage of total score) at p = 0.05.\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "for i, df in enumerate([df_outcome_baseline, df_outcome_1y, df_outcome_2y, df_outcome_4y]):\n",
    "    df['PERC_UPDRS3'] = df['NP3TOT']/df['UPDRS_TOT']\n",
    "\n",
    "    perc_updrs3_highsev = df['PERC_UPDRS3'][df[f'SEVERITY']==1].tolist()\n",
    "    perc_updrs3_lowsev = df['PERC_UPDRS3'][df[f'SEVERITY']==0].tolist()\n",
    "\n",
    "    if i == 0:\n",
    "        time = 'Baseline'\n",
    "    elif i < 3:\n",
    "        time = f'{i+1} years'\n",
    "    else:\n",
    "        time = '4 years'\n",
    "        \n",
    "    print(time, ':', round(ttest_ind(perc_updrs3_highsev, perc_updrs3_lowsev)[1], 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb46b117-aaa5-4313-a52f-06016e9af234",
   "metadata": {},
   "source": [
    "The distribution of the UPDRS-Score at Baseline and 1 year are also given in the supplementary materials. We tried to reproduce these figures. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "78d8ba4a-712a-4dff-9a75-c6d974f70bc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "f = plt.figure(figsize = (12, 4))\n",
    "gs = f.add_gridspec(1, 2)\n",
    "\n",
    "ax1 = f.add_subplot(gs[0, 0])\n",
    "ax1.set_xlim(0,120)\n",
    "ax1.set_ylim(0, 0.04)\n",
    "\n",
    "sns.histplot(df_outcome_baseline, x = 'UPDRS_TOT', axes=ax1, kde=True, stat='density')\n",
    "ax2 = f.add_subplot(gs[0, 1])\n",
    "ax2.set_xlim(0,140)\n",
    "sns.histplot(df_outcome_1y,\n",
    "             x = 'UPDRS_TOT', axes=ax2, kde=True, stat='density')\n",
    "            \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771052d3-f926-4b22-a09a-482f1adfa3ae",
   "metadata": {},
   "source": [
    "Reported plot in the original paper are shown below. \n",
    "\n",
    "<img src=\"images/outcome_distribution.png\" width=1000/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340604be-f9d1-4b20-a5ac-8b3442621e43",
   "metadata": {},
   "source": [
    "## Implementing machine learning models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "058c6c13-689a-48de-8279-43ecd04b5b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "from sklearn import model_selection, pipeline, preprocessing, impute, feature_selection, metrics, svm, linear_model, \\\n",
    "    ensemble\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "562f6c5a-223d-4e5c-90fb-36cb329b31e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StratifiedKFoldContinuous(model_selection.StratifiedKFold):\n",
    "    def __init__(self, n_splits=10, n_bins=3, shuffle=True, random_state=989):\n",
    "        self.n_bins = n_bins\n",
    "        super().__init__(n_splits=n_splits, shuffle=shuffle, random_state = random_state)\n",
    "\n",
    "    def split(self, X, y, groups=None):\n",
    "        yBinned = pd.qcut(y, self.n_bins, labels=False, duplicates='drop')\n",
    "        # yBinned.index = y.index\n",
    "        return super(StratifiedKFoldContinuous, self).split(X, yBinned, groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "23929d24-df7f-4be9-9d22-ef097314c401",
   "metadata": {},
   "outputs": [],
   "source": [
    "outer = model_selection.LeaveOneOut()\n",
    "inner = StratifiedKFoldContinuous()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "532e5040-af2d-4020-9935-c80c07d609ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(true, predict):\n",
    "    return np.sqrt(np.mean(np.square(true - predict)))\n",
    "\n",
    "\n",
    "def rsquare(true, predict):\n",
    "    ssTot = np.sum(np.square(true - np.mean(true)))\n",
    "    ssRes = np.sum(np.square(true - predict))\n",
    "    return 1 - (ssRes / (ssTot + np.finfo(float).eps))\n",
    "\n",
    "class RegressorPanel:\n",
    "    outer: model_selection.BaseCrossValidator\n",
    "    inner: model_selection.BaseCrossValidator\n",
    "\n",
    "    def __init__(self, data, target,\n",
    "                 outer=3,\n",
    "                 inner=5,\n",
    "                 metric_rs='rsquare',\n",
    "                 random_seed=432):\n",
    "        \"\"\"\n",
    "        Set of shallow learning models for regression.\n",
    "\n",
    "        :param data: dataframe of input data, or a path to a file containing the the input data (CSV or pkl)\n",
    "        :type data: pandas.DataFrame\n",
    "        :param target:\n",
    "        :type target: pandas.DataFrame\n",
    "        :param outer:\n",
    "        :type outer:\n",
    "        :param inner:\n",
    "        :type inner:\n",
    "        :param metric_rs: 'rsquare' or 'rmse'; which metric to use to select the best model in each random search\n",
    "        :type metric_rs: str\n",
    "        :param random_seed:\n",
    "        :type random_seed:\n",
    "        \"\"\"\n",
    "\n",
    "        self.data = data\n",
    "        self.target = target\n",
    "\n",
    "        if isinstance(outer, int):\n",
    "            self.outer = model_selection.KFold(n_splits=outer, shuffle=True, random_state=random_seed)\n",
    "        else:\n",
    "            self.outer = outer\n",
    "        if isinstance(inner, int):\n",
    "            self.inner = model_selection.KFold(n_splits=inner, shuffle=True, random_state=random_seed)\n",
    "        else:\n",
    "            self.inner = inner\n",
    "\n",
    "        self.model_dict = None\n",
    "        self.set_default_models()\n",
    "        self.preprocessing = None\n",
    "        self.set_default_preprocessing()\n",
    "        self.feature_selection = None\n",
    "        self.metric_rs = metric_rs\n",
    "        self.random_seed = random_seed\n",
    "\n",
    "    def set_models(self, *args):\n",
    "        \"\"\"\n",
    "        Select scikit-learn models to train. Pass in any number of tuples, where each tuple contains a scikit-learn\n",
    "        regression model class and the dictionary of hyperparameter ranges to search.\n",
    "        For example, set_models((sklearn.linear_model.Lasso, {'alpha': scipy.stats.uniform(0.1, 10.0),\n",
    "                                                            'max_iter': scipy.stats.randint(500, 5000)},\n",
    "                                sklearn.ensemble.RandomForestRegressor, {'n_estimators': stats.randint(200, 12000),\n",
    "                                                                      'min_samples_split': stats.uniform(0.01, 0.5),\n",
    "                                                                      'min_samples_leaf': stats.randint(1, 6),\n",
    "                                                                      'max_depth': stats.randint(1, 10)}\n",
    "                                )\n",
    "        If models are not specified, the default models and hyperparameters will be used.\n",
    "\n",
    "        :param args: tuples containing (regression model, hyperparameter dictionary) pairs\n",
    "        :type args: tuple\n",
    "        :return:\n",
    "        :rtype:\n",
    "        \"\"\"\n",
    "        self.model_dict = {model_tupple[0].__name__: model_tupple for model_tupple in args}\n",
    "\n",
    "    def set_default_models(self):\n",
    "        self.model_dict = {'ElasticNet': (linear_model.ElasticNet(max_iter=5000),\n",
    "                                            {'alpha': np.logspace(0, 1.5, 1000),\n",
    "                                             'l1_ratio': stats.uniform(0, 1.0)}\n",
    "                                            ),\n",
    "                             'LinearSVR': (svm.LinearSVR(tol=0.001, max_iter=50000),\n",
    "                                           {'C': np.logspace(-3, 0, 1000, base=10),\n",
    "                                            'epsilon': np.logspace(-2, 0, 1000),\n",
    "                                            'loss': ['epsilon_insensitive', 'squared_epsilon_insensitive']}\n",
    "                                           ),\n",
    "                             'GradientBoostingRegressor': (ensemble.GradientBoostingRegressor(loss='squared_error',\n",
    "                                                                                              criterion='friedman_mse'),\n",
    "                                                           {'learning_rate': np.logspace(-2, -1, 1000),\n",
    "                                                            'n_estimators': np.linspace(10, 1000, 1000,\n",
    "                                                                                        dtype=int),\n",
    "                                                            'min_samples_split': stats.uniform(0.1, 0.7),\n",
    "                                                            'min_samples_leaf': stats.randint(1, 6),\n",
    "                                                            'max_depth': stats.randint(1, 4)}\n",
    "                                                           ),\n",
    "                             'RandomForestRegressor': (ensemble.RandomForestRegressor(criterion='squared_error', random_state=432),\n",
    "                                                        {'n_estimators': np.logspace(1, 3, 100).astype(int),\n",
    "                                                        'min_samples_split': stats.uniform(0.01, 0.5),\n",
    "                                                        'min_samples_leaf': stats.randint(1, 6),\n",
    "                                                        'max_depth': stats.randint(1, 8)}\n",
    "                                                       )\n",
    "                             }\n",
    "\n",
    "    def set_preprocessing(self, pipeline):\n",
    "        \"\"\"\n",
    "        Set the input data preprocessing  pipeline that will be applied to all data before it is passed into a\n",
    "        regression model. Typically, this pipeline would contain some kind of scaling/normalization and NaN imputer.\n",
    "        If no pipeline is specified, the default will be used:\n",
    "            pipeline.Pipeline([('scaler', preprocessing.StandardScaler()),\n",
    "                                  ('imputer', impute.SimpleImputer()),\n",
    "                                  ('selector', feature_selection.SelectKBest(feature_selection.f_regression))])\n",
    "\n",
    "        :param pipeline: a scikit-learn pipeline object.\n",
    "        :type pipeline: sklearn.pipeline.Pipeline\n",
    "        :return:\n",
    "        :rtype:\n",
    "        \"\"\"\n",
    "        self.preprocessing = pipeline\n",
    "\n",
    "    def set_default_preprocessing(self):\n",
    "        self.preprocessing = pipeline.Pipeline([('StandardScaler', preprocessing.StandardScaler()),\n",
    "                                                ('SimpleImputer', impute.SimpleImputer())])\n",
    "\n",
    "    def set_feature_selection(self, feature_selection_tupple):\n",
    "        \"\"\"\n",
    "        Add a feature selection method that will be used after preprocessing and before model fitting. Takes a tuple\n",
    "        containing (sklearn feature selection class, dictionary of hyperparams for this selector)\n",
    "\n",
    "        :param feature_selection_tupple: tuple containing (feature selection class, dictionary of hyperparams)\n",
    "        :type feature_selection_tupple: tuple\n",
    "        :return:\n",
    "        :rtype:\n",
    "        \"\"\"\n",
    "        self.feature_selection = feature_selection_tupple\n",
    "\n",
    "    def run_single_model(self, model_name, n_iters=100, n_jobs=1):\n",
    "        \"\"\"\n",
    "        Run a random search on one model\n",
    "\n",
    "        :param model_name: key referring to one of the models in self.model_dict\n",
    "        :type model_name: str\n",
    "        :param n_iters: number of iterations in the random search\n",
    "        :type n_iters: int\n",
    "        :param n_jobs: number of parallel jobs\n",
    "        :type n_jobs: int\n",
    "        :return: result_dict: dictionary containing mean train, val, and test RMSE and Rsquare; cv_score_dict:\n",
    "        dictionary containing the exhaustive results returned by sklearn.model_selection.cross_validate\n",
    "        :rtype: dict, dict\n",
    "        \"\"\"\n",
    "        if model_name not in self.model_dict.keys():\n",
    "            raise ValueError('Incorrect model name: {}'.format(model_name)) # Test for name in model dictionnary\n",
    "\n",
    "        pipe = copy.deepcopy(self.preprocessing)\n",
    "\n",
    "        if self.feature_selection is not None: # If feature selection ? \n",
    "            selector_name = self.feature_selection[0].__class__.__name__\n",
    "            pipe.steps.append((selector_name, self.feature_selection[0]))\n",
    "\n",
    "        pipe.steps.append((model_name, self.model_dict[model_name][0]))\n",
    "\n",
    "        param_dict_orig = self.model_dict[model_name][1]\n",
    "        # Append the model name to the hyperparam keys so that RandomSearch can recognize them\n",
    "        param_dict = {model_name + '__' + key: val for key, val in param_dict_orig.items()}\n",
    "\n",
    "        if self.feature_selection is not None:\n",
    "            select_orig_param_dict = self.feature_selection[1]\n",
    "            select_param_dict = {selector_name + '__' + key: val for key, val in select_orig_param_dict.items()}\n",
    "            param_dict = dict(param_dict, **select_param_dict)\n",
    "\n",
    "        score_dict = {'rmse': metrics.make_scorer(rmse,\n",
    "                                                   greater_is_better=False),\n",
    "                       'rsquare': metrics.make_scorer(rsquare,\n",
    "                                                      greater_is_better=True)}\n",
    "\n",
    "        random = model_selection.RandomizedSearchCV(pipe, param_dict,\n",
    "                                                    scoring=score_dict,\n",
    "                                                    cv=self.inner,\n",
    "                                                    n_iter=n_iters,\n",
    "                                                    return_train_score=True,\n",
    "                                                    n_jobs=n_jobs,\n",
    "                                                    random_state=self.random_seed,\n",
    "                                                    refit=self.metric_rs)\n",
    "\n",
    "        cv_score_dict = model_selection.cross_validate(random, X=self.data, y=self.target, cv=self.outer,\n",
    "                                                      groups=None,\n",
    "                                                      scoring=score_dict,\n",
    "                                                      return_train_score=True, return_estimator=True)\n",
    "        if hasattr(self.outer, 'n_splits'):\n",
    "            n_outer_splits = self.outer.n_splits\n",
    "        elif hasattr(self.outer, 'get_n_splits'):\n",
    "            n_outer_splits = self.outer.get_n_splits(self.data)\n",
    "        elif hasattr(self.outer, '__len__'):\n",
    "            n_outer_splits = self.outer.__len__()\n",
    "            \n",
    "        rmse_train = -cv_score_dict['train_rmse']\n",
    "        rsquare_train = cv_score_dict['train_rsquare']\n",
    "        best_model_idx = [cv_score_dict['estimator'][n_fold].best_index_ for n_fold in range(n_outer_splits)]\n",
    "        \n",
    "        rmse_valid = [-cv_score_dict['estimator'][n_fold].cv_results_['mean_test_rmse'][best_model_idx[n_fold]] for n_fold in\n",
    "                     range(n_outer_splits)]\n",
    "        rsquare_valid = [cv_score_dict['estimator'][n_fold].cv_results_['mean_test_rsquare'][best_model_idx[n_fold]] for\n",
    "                        n_fold in range(n_outer_splits)]\n",
    "        \n",
    "        rmse_test = -cv_score_dict['test_rmse']\n",
    "        rsquare_test = cv_score_dict['test_rsquare']\n",
    "        \n",
    "        result_dict = {'mean_train_rmse': np.mean(rmse_train),\n",
    "                       'std_train_rmse': np.std(rmse_train),\n",
    "                       'mean_val_rmse': np.mean(rmse_valid),\n",
    "                       'std_val_rmse': np.std(rmse_valid),\n",
    "                       'mean_test_rmse': np.mean(rmse_test),\n",
    "                       'std_test_rmse': np.std(rmse_test),\n",
    "                       'mean_train_rsquare': np.mean(rsquare_train),\n",
    "                       'std_train_rsquare': np.std(rsquare_train),\n",
    "                       'mean_val_rsquare': np.mean(rsquare_valid),\n",
    "                       'std_val_rsquare': np.std(rsquare_valid),\n",
    "                       'mean_test_rsquare': np.mean(rsquare_test),\n",
    "                       'std_test_rsquare': np.std(rsquare_test)\n",
    "                       }\n",
    "\n",
    "        return result_dict, cv_score_dict\n",
    "\n",
    "    def run_all_models(self, n_iters=100, verbose=True, n_jobs=1):\n",
    "        \"\"\"\n",
    "        Run random hyperparam searches on all models\n",
    "\n",
    "        :param n_iters: number of random search iterations\n",
    "        :type n_iters: int\n",
    "        :param verbose: flag for verbose output\n",
    "        :type verbose: bool\n",
    "        :param n_jobs: number of parallel jobs\n",
    "        :type n_jobs: int\n",
    "        :return: result dataframe\n",
    "        :rtype: pandas.DataFrame\n",
    "        \"\"\"\n",
    "\n",
    "        if verbose:\n",
    "            #print('Writing results to {}'.format(self.strOutputDir))\n",
    "            if hasattr(self.outer, 'n_splits'):\n",
    "                n_outer_splits = self.outer.n_splits\n",
    "            elif hasattr(self.outer, 'get_n_splits'):\n",
    "                n_outer_splits = self.outer.get_n_splits(self.data)\n",
    "            elif hasattr(self.outer, '__len__'):\n",
    "                n_outer_splits = self.outer.__len__()\n",
    "            else:\n",
    "                n_outer_splits = '?'\n",
    "\n",
    "            if hasattr(self.inner, 'n_splits'):\n",
    "                n_inner_splits = self.inner.n_splits\n",
    "            elif hasattr(self.inner, '__len__'):\n",
    "                n_inner_splits = self.inner.__len__()\n",
    "            else:\n",
    "                n_inner_splits = '?'\n",
    "                \n",
    "            print('Using {} outer folds and {} inner folds'.format(n_outer_splits, n_inner_splits))\n",
    "            print('{} subjects, {} features'.format(self.data.shape[0], self.data.shape[1]))\n",
    "\n",
    "        result_all_dict = {}\n",
    "\n",
    "        for model_name, model_tupple in self.model_dict.items():\n",
    "            if verbose:\n",
    "                print('Training model {}'.format(model_name), flush=True)\n",
    "            result_dict, cv_score_dict = self.run_single_model(model_name, n_iters, n_jobs)\n",
    "            result_all_dict[model_name] = result_dict\n",
    "            \n",
    "            if verbose:\n",
    "                print('{}: \\nmean val rmse: {} \\nmean test rmse: {}' \\\n",
    "                      '\\nmean val rsquare: {} \\nmean test rsquare: {}'.format(model_name,\n",
    "                                                                              result_dict['mean_val_rmse'],\n",
    "                                                                              result_dict['mean_test_rmse'],\n",
    "                                                                              result_dict['mean_val_rsquare'],\n",
    "                                                                              result_dict['mean_test_rsquare']),\n",
    "                      flush=True)\n",
    "\n",
    "        df_summary = pd.DataFrame(result_all_dict).T\n",
    "\n",
    "        return df_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "e4b61e2b-43b0-4874-8811-8564ee9e982c",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = df_outcome_baseline['UPDRS_TOT'].to_numpy(copy=True)\n",
    "data = df_all_features_cohort_baseline.drop(['EVENT_ID'], axis=1).to_numpy(copy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "3e204447-71b0-46a6-98d0-6e25b9b14424",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 102 outer folds and 10 inner folds\n",
      "102 subjects, 153 features\n",
      "Training model ElasticNet\n",
      "ElasticNet: \n",
      "mean val rmse: 14.431224845205211 \n",
      "mean test rmse: 11.932822836388056\n",
      "mean val rsquare: 0.05749933415262926 \n",
      "mean test rsquare: -9.581668985374476e+17\n",
      "Training model LinearSVR\n",
      "LinearSVR: \n",
      "mean val rmse: 14.36468470786295 \n",
      "mean test rmse: 11.672801119736619\n",
      "mean val rsquare: 0.061419177003206066 \n",
      "mean test rsquare: -9.372424125580406e+17\n",
      "Training model GradientBoostingRegressor\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[106], line 8\u001b[0m\n\u001b[1;32m      1\u001b[0m regressors \u001b[38;5;241m=\u001b[39m RegressorPanel(data,target,\n\u001b[1;32m      2\u001b[0m                             outer\u001b[38;5;241m=\u001b[39mouter,\n\u001b[1;32m      3\u001b[0m                             inner\u001b[38;5;241m=\u001b[39minner)\n\u001b[1;32m      5\u001b[0m regressors\u001b[38;5;241m.\u001b[39mset_feature_selection((feature_selection\u001b[38;5;241m.\u001b[39mSelectPercentile(feature_selection\u001b[38;5;241m.\u001b[39mf_regression),\n\u001b[1;32m      6\u001b[0m                                   {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpercentile\u001b[39m\u001b[38;5;124m'\u001b[39m: np\u001b[38;5;241m.\u001b[39mlinspace(\u001b[38;5;241m0.05\u001b[39m, \u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m10\u001b[39m)}))\n\u001b[0;32m----> 8\u001b[0m df_summary \u001b[38;5;241m=\u001b[39m \u001b[43mregressors\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_all_models\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m df_summary\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./outputs/results_feature_predition_reho_fmriprep.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[98], line 262\u001b[0m, in \u001b[0;36mRegressorPanel.run_all_models\u001b[0;34m(self, n_iters, verbose, n_jobs)\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verbose:\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining model \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(model_name), flush\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 262\u001b[0m result_dict, cv_score_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_single_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_iters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    263\u001b[0m result_all_dict[model_name] \u001b[38;5;241m=\u001b[39m result_dict\n\u001b[1;32m    265\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verbose:\n",
      "Cell \u001b[0;32mIn[98], line 183\u001b[0m, in \u001b[0;36mRegressorPanel.run_single_model\u001b[0;34m(self, model_name, n_iters, n_jobs)\u001b[0m\n\u001b[1;32m    169\u001b[0m score_dict \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrmse\u001b[39m\u001b[38;5;124m'\u001b[39m: metrics\u001b[38;5;241m.\u001b[39mmake_scorer(rmse,\n\u001b[1;32m    170\u001b[0m                                            greater_is_better\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[1;32m    171\u001b[0m                \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrsquare\u001b[39m\u001b[38;5;124m'\u001b[39m: metrics\u001b[38;5;241m.\u001b[39mmake_scorer(rsquare,\n\u001b[1;32m    172\u001b[0m                                               greater_is_better\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)}\n\u001b[1;32m    174\u001b[0m random \u001b[38;5;241m=\u001b[39m model_selection\u001b[38;5;241m.\u001b[39mRandomizedSearchCV(pipe, param_dict,\n\u001b[1;32m    175\u001b[0m                                             scoring\u001b[38;5;241m=\u001b[39mscore_dict,\n\u001b[1;32m    176\u001b[0m                                             cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minner,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    180\u001b[0m                                             random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrandom_seed,\n\u001b[1;32m    181\u001b[0m                                             refit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetric_rs)\n\u001b[0;32m--> 183\u001b[0m cv_score_dict \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_selection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_validate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrandom\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mouter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[43m                                              \u001b[49m\u001b[43mgroups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[43m                                              \u001b[49m\u001b[43mscoring\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscore_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[43m                                              \u001b[49m\u001b[43mreturn_train_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_estimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mouter, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_splits\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    188\u001b[0m     n_outer_splits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mouter\u001b[38;5;241m.\u001b[39mn_splits\n",
      "File \u001b[0;32m~/miniforge3/envs/livingpark_env/lib/python3.10/site-packages/sklearn/utils/_param_validation.py:211\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    207\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    208\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    209\u001b[0m         )\n\u001b[1;32m    210\u001b[0m     ):\n\u001b[0;32m--> 211\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    213\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    218\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    219\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    220\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    221\u001b[0m     )\n",
      "File \u001b[0;32m~/miniforge3/envs/livingpark_env/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:309\u001b[0m, in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, return_indices, error_score)\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[38;5;66;03m# We clone the estimator to make sure that all the folds are\u001b[39;00m\n\u001b[1;32m    307\u001b[0m \u001b[38;5;66;03m# independent, and that it is pickle-able.\u001b[39;00m\n\u001b[1;32m    308\u001b[0m parallel \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39mn_jobs, verbose\u001b[38;5;241m=\u001b[39mverbose, pre_dispatch\u001b[38;5;241m=\u001b[39mpre_dispatch)\n\u001b[0;32m--> 309\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscorers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfit_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_train_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_train_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_times\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    322\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_estimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_estimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    323\u001b[0m \u001b[43m        \u001b[49m\u001b[43merror_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merror_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    324\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    325\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\n\u001b[1;32m    326\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    328\u001b[0m _warn_or_raise_about_fit_failures(results, error_score)\n\u001b[1;32m    330\u001b[0m \u001b[38;5;66;03m# For callable scoring, the return type is only know after calling. If the\u001b[39;00m\n\u001b[1;32m    331\u001b[0m \u001b[38;5;66;03m# return type is a dictionary, the error scores can now be inserted with\u001b[39;00m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;66;03m# the correct key.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/livingpark_env/lib/python3.10/site-packages/sklearn/utils/parallel.py:65\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     60\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     61\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     62\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     64\u001b[0m )\n\u001b[0;32m---> 65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/livingpark_env/lib/python3.10/site-packages/joblib/parallel.py:1863\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1861\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[1;32m   1862\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 1863\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1865\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[1;32m   1866\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[1;32m   1867\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[1;32m   1868\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[1;32m   1869\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[1;32m   1870\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[0;32m~/miniforge3/envs/livingpark_env/lib/python3.10/site-packages/joblib/parallel.py:1792\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1790\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1791\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1792\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1793\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1794\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[0;32m~/miniforge3/envs/livingpark_env/lib/python3.10/site-packages/sklearn/utils/parallel.py:127\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    125\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[0;32m--> 127\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/livingpark_env/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:732\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[1;32m    730\u001b[0m         estimator\u001b[38;5;241m.\u001b[39mfit(X_train, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[1;32m    731\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 732\u001b[0m         \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    734\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m    735\u001b[0m     \u001b[38;5;66;03m# Note fit time as time until error\u001b[39;00m\n\u001b[1;32m    736\u001b[0m     fit_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n",
      "File \u001b[0;32m~/miniforge3/envs/livingpark_env/lib/python3.10/site-packages/sklearn/base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1144\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1147\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1149\u001b[0m     )\n\u001b[1;32m   1150\u001b[0m ):\n\u001b[0;32m-> 1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/livingpark_env/lib/python3.10/site-packages/sklearn/model_selection/_search.py:898\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    892\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[1;32m    893\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[1;32m    894\u001b[0m     )\n\u001b[1;32m    896\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[0;32m--> 898\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    900\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[1;32m    901\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[1;32m    902\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/miniforge3/envs/livingpark_env/lib/python3.10/site-packages/sklearn/model_selection/_search.py:1806\u001b[0m, in \u001b[0;36mRandomizedSearchCV._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1804\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[1;32m   1805\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search n_iter candidates from param_distributions\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1806\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1807\u001b[0m \u001b[43m        \u001b[49m\u001b[43mParameterSampler\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1808\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_distributions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom_state\u001b[49m\n\u001b[1;32m   1809\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1810\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/livingpark_env/lib/python3.10/site-packages/sklearn/model_selection/_search.py:845\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    837\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    838\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m    839\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    840\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    841\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[1;32m    842\u001b[0m         )\n\u001b[1;32m    843\u001b[0m     )\n\u001b[0;32m--> 845\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    846\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    847\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    848\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    849\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    850\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    851\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    852\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    853\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    854\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    855\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    856\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    857\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    858\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    859\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    863\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    864\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    865\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    866\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    867\u001b[0m     )\n",
      "File \u001b[0;32m~/miniforge3/envs/livingpark_env/lib/python3.10/site-packages/sklearn/utils/parallel.py:65\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     60\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     61\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     62\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     64\u001b[0m )\n\u001b[0;32m---> 65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/livingpark_env/lib/python3.10/site-packages/joblib/parallel.py:1863\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1861\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[1;32m   1862\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 1863\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1865\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[1;32m   1866\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[1;32m   1867\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[1;32m   1868\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[1;32m   1869\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[1;32m   1870\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[0;32m~/miniforge3/envs/livingpark_env/lib/python3.10/site-packages/joblib/parallel.py:1792\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1790\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1791\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1792\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1793\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1794\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[0;32m~/miniforge3/envs/livingpark_env/lib/python3.10/site-packages/sklearn/utils/parallel.py:127\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    125\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[0;32m--> 127\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/livingpark_env/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:732\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[1;32m    730\u001b[0m         estimator\u001b[38;5;241m.\u001b[39mfit(X_train, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[1;32m    731\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 732\u001b[0m         \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    734\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m    735\u001b[0m     \u001b[38;5;66;03m# Note fit time as time until error\u001b[39;00m\n\u001b[1;32m    736\u001b[0m     fit_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n",
      "File \u001b[0;32m~/miniforge3/envs/livingpark_env/lib/python3.10/site-packages/sklearn/base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1144\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1147\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1149\u001b[0m     )\n\u001b[1;32m   1150\u001b[0m ):\n\u001b[0;32m-> 1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/livingpark_env/lib/python3.10/site-packages/sklearn/pipeline.py:420\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    419\u001b[0m         fit_params_last_step \u001b[38;5;241m=\u001b[39m fit_params_steps[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m]]\n\u001b[0;32m--> 420\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_final_estimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params_last_step\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    422\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/miniforge3/envs/livingpark_env/lib/python3.10/site-packages/sklearn/base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1144\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1147\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1149\u001b[0m     )\n\u001b[1;32m   1150\u001b[0m ):\n\u001b[0;32m-> 1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/livingpark_env/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:525\u001b[0m, in \u001b[0;36mBaseGradientBoosting.fit\u001b[0;34m(self, X, y, sample_weight, monitor)\u001b[0m\n\u001b[1;32m    522\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_resize_state()\n\u001b[1;32m    524\u001b[0m \u001b[38;5;66;03m# fit the boosting stages\u001b[39;00m\n\u001b[0;32m--> 525\u001b[0m n_stages \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_stages\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    528\u001b[0m \u001b[43m    \u001b[49m\u001b[43mraw_predictions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    529\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    530\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_rng\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    531\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    532\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    533\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    534\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbegin_at_stage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    535\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmonitor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    536\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    538\u001b[0m \u001b[38;5;66;03m# change shape of arrays after fit (early-stopping or additional ests)\u001b[39;00m\n\u001b[1;32m    539\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_stages \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]:\n",
      "File \u001b[0;32m~/miniforge3/envs/livingpark_env/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:603\u001b[0m, in \u001b[0;36mBaseGradientBoosting._fit_stages\u001b[0;34m(self, X, y, raw_predictions, sample_weight, random_state, X_val, y_val, sample_weight_val, begin_at_stage, monitor)\u001b[0m\n\u001b[1;32m    596\u001b[0m         initial_loss \u001b[38;5;241m=\u001b[39m loss_(\n\u001b[1;32m    597\u001b[0m             y[\u001b[38;5;241m~\u001b[39msample_mask],\n\u001b[1;32m    598\u001b[0m             raw_predictions[\u001b[38;5;241m~\u001b[39msample_mask],\n\u001b[1;32m    599\u001b[0m             sample_weight[\u001b[38;5;241m~\u001b[39msample_mask],\n\u001b[1;32m    600\u001b[0m         )\n\u001b[1;32m    602\u001b[0m \u001b[38;5;66;03m# fit next stage of trees\u001b[39;00m\n\u001b[0;32m--> 603\u001b[0m raw_predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_stage\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    604\u001b[0m \u001b[43m    \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    605\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    606\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    607\u001b[0m \u001b[43m    \u001b[49m\u001b[43mraw_predictions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    608\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    609\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    610\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    611\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_csc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    612\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_csr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    615\u001b[0m \u001b[38;5;66;03m# track loss\u001b[39;00m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m do_oob:\n",
      "File \u001b[0;32m~/miniforge3/envs/livingpark_env/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:245\u001b[0m, in \u001b[0;36mBaseGradientBoosting._fit_stage\u001b[0;34m(self, i, X, y, raw_predictions, sample_weight, sample_mask, random_state, X_csc, X_csr)\u001b[0m\n\u001b[1;32m    242\u001b[0m     sample_weight \u001b[38;5;241m=\u001b[39m sample_weight \u001b[38;5;241m*\u001b[39m sample_mask\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat64)\n\u001b[1;32m    244\u001b[0m X \u001b[38;5;241m=\u001b[39m X_csr \u001b[38;5;28;01mif\u001b[39;00m X_csr \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m X\n\u001b[0;32m--> 245\u001b[0m \u001b[43mtree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresidual\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;66;03m# update tree leaves\u001b[39;00m\n\u001b[1;32m    248\u001b[0m loss\u001b[38;5;241m.\u001b[39mupdate_terminal_regions(\n\u001b[1;32m    249\u001b[0m     tree\u001b[38;5;241m.\u001b[39mtree_,\n\u001b[1;32m    250\u001b[0m     X,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    257\u001b[0m     k\u001b[38;5;241m=\u001b[39mk,\n\u001b[1;32m    258\u001b[0m )\n",
      "File \u001b[0;32m~/miniforge3/envs/livingpark_env/lib/python3.10/site-packages/sklearn/base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1144\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1147\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1149\u001b[0m     )\n\u001b[1;32m   1150\u001b[0m ):\n\u001b[0;32m-> 1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/livingpark_env/lib/python3.10/site-packages/sklearn/tree/_classes.py:1320\u001b[0m, in \u001b[0;36mDecisionTreeRegressor.fit\u001b[0;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[1;32m   1290\u001b[0m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   1291\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m   1292\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Build a decision tree regressor from the training set (X, y).\u001b[39;00m\n\u001b[1;32m   1293\u001b[0m \n\u001b[1;32m   1294\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1317\u001b[0m \u001b[38;5;124;03m        Fitted estimator.\u001b[39;00m\n\u001b[1;32m   1318\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1320\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1321\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/miniforge3/envs/livingpark_env/lib/python3.10/site-packages/sklearn/tree/_classes.py:372\u001b[0m, in \u001b[0;36mBaseDecisionTree._fit\u001b[0;34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[0m\n\u001b[1;32m    366\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    367\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of labels=\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m does not match number of samples=\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    368\u001b[0m         \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mlen\u001b[39m(y), n_samples)\n\u001b[1;32m    369\u001b[0m     )\n\u001b[1;32m    371\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 372\u001b[0m     sample_weight \u001b[38;5;241m=\u001b[39m \u001b[43m_check_sample_weight\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDOUBLE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m expanded_class_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    375\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniforge3/envs/livingpark_env/lib/python3.10/site-packages/sklearn/utils/validation.py:1838\u001b[0m, in \u001b[0;36m_check_sample_weight\u001b[0;34m(sample_weight, X, dtype, copy, only_non_negative)\u001b[0m\n\u001b[1;32m   1836\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1837\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m [np\u001b[38;5;241m.\u001b[39mfloat64, np\u001b[38;5;241m.\u001b[39mfloat32]\n\u001b[0;32m-> 1838\u001b[0m sample_weight \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1839\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1840\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1841\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1842\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1843\u001b[0m \u001b[43m    \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1844\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1845\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msample_weight\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1846\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1847\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sample_weight\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   1848\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSample weights must be 1D array or scalar\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniforge3/envs/livingpark_env/lib/python3.10/site-packages/sklearn/utils/validation.py:959\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    953\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    954\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    955\u001b[0m             \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[1;32m    956\u001b[0m         )\n\u001b[1;32m    958\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m force_all_finite:\n\u001b[0;32m--> 959\u001b[0m         \u001b[43m_assert_all_finite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    960\u001b[0m \u001b[43m            \u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    961\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    962\u001b[0m \u001b[43m            \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    963\u001b[0m \u001b[43m            \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    964\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    966\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ensure_min_samples \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    967\u001b[0m     n_samples \u001b[38;5;241m=\u001b[39m _num_samples(array)\n",
      "File \u001b[0;32m~/miniforge3/envs/livingpark_env/lib/python3.10/site-packages/sklearn/utils/validation.py:119\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;66;03m# First try an O(n) time, O(1) space solution for the common case that\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;66;03m# everything is finite; fall back to O(n) space `np.isinf/isnan` or custom\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;66;03m# Cython implementation to prevent false positives and provide a detailed\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;66;03m# error message.\u001b[39;00m\n\u001b[0;32m--> 119\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m np\u001b[38;5;241m.\u001b[39merrstate(over\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    120\u001b[0m     first_pass_isfinite \u001b[38;5;241m=\u001b[39m xp\u001b[38;5;241m.\u001b[39misfinite(xp\u001b[38;5;241m.\u001b[39msum(X))\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n",
      "File \u001b[0;32m~/miniforge3/envs/livingpark_env/lib/python3.10/site-packages/numpy/core/_ufunc_config.py:431\u001b[0m, in \u001b[0;36merrstate.__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    430\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__enter__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 431\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moldstate \u001b[38;5;241m=\u001b[39m \u001b[43mseterr\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    432\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _Unspecified:\n\u001b[1;32m    433\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moldcall \u001b[38;5;241m=\u001b[39m seterrcall(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall)\n",
      "File \u001b[0;32m~/miniforge3/envs/livingpark_env/lib/python3.10/site-packages/numpy/core/_ufunc_config.py:111\u001b[0m, in \u001b[0;36mseterr\u001b[0;34m(all, divide, over, under, invalid)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;124;03mSet how floating-point errors are handled.\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    107\u001b[0m \n\u001b[1;32m    108\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    110\u001b[0m pyvals \u001b[38;5;241m=\u001b[39m umath\u001b[38;5;241m.\u001b[39mgeterrobj()\n\u001b[0;32m--> 111\u001b[0m old \u001b[38;5;241m=\u001b[39m \u001b[43mgeterr\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m divide \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    114\u001b[0m     divide \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mall\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m old[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdivide\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "regressors = RegressorPanel(data,target,\n",
    "                            outer=outer,\n",
    "                            inner=inner)\n",
    "\n",
    "regressors.set_feature_selection((feature_selection.SelectPercentile(feature_selection.f_regression),\n",
    "                                  {'percentile': np.linspace(0.05, 0.5, 10)}))\n",
    "\n",
    "df_summary = regressors.run_all_models()\n",
    "df_summary.to_csv('./outputs/results_feature_predition_falff_fmriprep.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05bf5ecc-0139-413b-8272-b489d1d836a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cdfddfc-56c7-4ee8-9d40-bf3262d5291d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755f453c-9a7a-45fa-ab4d-afa1d39a3e6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e100d636-070f-4698-9346-db5fb97de6f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e760ba9-1011-4065-9059-777422c27e69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0b99fb-738c-42b2-9ee0-0aee1a106335",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa3d325-87b3-4ff5-b9da-35d1bb8f6b88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65095408-c419-4646-bb0c-a902730880b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c960262-db0f-41a8-9435-3c6bcfe81e35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab40d05-a77e-49d1-a5ca-a1c186e33f34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd8b3e2-df2b-4635-914b-79003e44f1e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683f69c9-4226-4328-87e1-7597db322c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import sys\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "from ppmiutils import dataset\n",
    "import numpy as np\n",
    "import nilearn.datasets, regions, image\n",
    "from sklearn import metrics, model_selection\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "atlas_dict = {'basc197': basc_atlas['scale197'],\n",
    "                   'basc444': basc_atlas['scale444'],\n",
    "                   'schaefer': schaefer_atlas\n",
    "                   }\n",
    "\n",
    "def get_prediction_results(inputs, target, result_all_dict, threshold):\n",
    "    results = []\n",
    "    for model_name in ['ElasticNet', 'LinearSVR', 'GradientBoostingRegressor', 'RandomForestRegressor']:\n",
    "        #model_dict = \n",
    "        outer = model_selection.LeaveOneOut()\n",
    "        \n",
    "        pred = np.zeros(inputs.shape[0])\n",
    "        rsquare_train = np.zeros_like(pred)\n",
    "        rmse_train = np.zeros_like(pred)\n",
    "        r2_valid = np.zeros_like(pred)\n",
    "        rmse_valid = np.zeros_like(pred)\n",
    "\n",
    "        for i, (train_idx, test_idx) in enumerate(outer.split(inputs, target)):\n",
    "            inputs_test = inputs.iloc[test_idx]\n",
    "            inputs_train = inputs.iloc[train_idx]\n",
    "            model = model_dict['estimator'][i].best_estimator_\n",
    "            pred[i] = model.predict(inputs_test.astype(np.float64))\n",
    "            pred_train = model.predict(inputs_train.astype(np.float64))\n",
    "            rsquare_train[i] = metrics.r2_score(target[train_idx], pred_train)\n",
    "            rmse_train[i] = np.sqrt(metrics.mean_squared_error(target[train_idx], pred_train))\n",
    "\n",
    "            search_results = pd.DataFrame(model_dict['estimator'][i].cv_results_)\n",
    "            best_model = model_dict['estimator'][i].best_index_\n",
    "            rsquare_valid[i] = search_results['mean_test_rsquare'].iloc[best_model]\n",
    "            rmse_valid[i] = -search_results['mean_test_rmse'].iloc[best_model]\n",
    "\n",
    "        true_class = target > threshold\n",
    "        pred_class = pred > threshold\n",
    "        trueneg, falsepos, falseneg, truepos = metrics.confusion_matrix(true_class,\n",
    "                                                                        pred_class).ravel()\n",
    "\n",
    "        results += [{'Model': model,\n",
    "                        'Test R2' : metrics.r2_score(target, pred),\n",
    "                        'Test RMSE': np.sqrt(metrics.mean_squared_error(target, pred)),\n",
    "                        'Test AUC': metrics.roc_auc_score(true_class, pred_class),\n",
    "                        'Test precision': metrics.precision_score(true_class, pred_class),\n",
    "                        'Test recall': metrics.recall_score(true_class, pred_class),\n",
    "                        'Test accuracy': metrics.accuracy_score(true_class, pred_class),\n",
    "                        'Test f1': metrics.f1_score(true_class, pred_class),\n",
    "                        'Test NPV': trueneg / (trueneg + falseneg),\n",
    "                        'Test specificity': trueneg / (trueneg + falsepos),\n",
    "                        'Val Mean R2': np.mean(rsquare_valid),\n",
    "                        'Val Std R2': np.std(rsquare_valid),\n",
    "                        'Val Mean RMSE': np.mean(rmse_valid),\n",
    "                        'Val Std RMSE': np.std(rmse_valid),\n",
    "                        'Train Mean R2': np.mean(rsquare_train),\n",
    "                        'Train Std R2': np.std(rsquare_train),\n",
    "                        'Train Mean RMSE': np.mean(rmse_train),\n",
    "                        'Train Std RMSE': np.std(rmse_train)}]\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79ddb70",
   "metadata": {},
   "source": [
    "## Not included in Notebook: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730e500c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_list = [3107,\n",
    " 3108,\n",
    " 3113,\n",
    " 3116,\n",
    " 3123,\n",
    " 3124,\n",
    " 3125,\n",
    " 3126,\n",
    " 3127,\n",
    " 3128,\n",
    " 3130,\n",
    " 3134,\n",
    " 3327,\n",
    " 3332,\n",
    " 3352,\n",
    " 3354,\n",
    " 3359,\n",
    " 3360,\n",
    " 3364,\n",
    " 3365,\n",
    " 3366,\n",
    " 3367,\n",
    " 3371,\n",
    " 3372,\n",
    " 3373,\n",
    " 3375,\n",
    " 3378,\n",
    " 3380,\n",
    " 3383,\n",
    " 3385,\n",
    " 3386,\n",
    " 3387,\n",
    " 3392,\n",
    " 3552,\n",
    " 3557,\n",
    " 3567,\n",
    " 3574,\n",
    " 3575,\n",
    " 3577,\n",
    " 3586,\n",
    " 3587,\n",
    " 3588,\n",
    " 3589,\n",
    " 3591,\n",
    " 3592,\n",
    " 3593,\n",
    " 3760,\n",
    " 3800,\n",
    " 3808,\n",
    " 3814,\n",
    " 3815,\n",
    " 3818,\n",
    " 3819,\n",
    " 3822,\n",
    " 3823,\n",
    " 3825,\n",
    " 3826,\n",
    " 3828,\n",
    " 3829,\n",
    " 3830,\n",
    " 3831,\n",
    " 3832,\n",
    " 3834,\n",
    " 3838,\n",
    " 3870,\n",
    " 4020,\n",
    " 4024,\n",
    " 4026,\n",
    " 4030,\n",
    " 4034,\n",
    " 4035,\n",
    " 40366,\n",
    " 4038,\n",
    " 40533,\n",
    " 50485,\n",
    " 50901,\n",
    " 51632,\n",
    " 51731,\n",
    " 52678,\n",
    " 53060,\n",
    " 55395,\n",
    " 70463]\n",
    "\n",
    "ses_list = ['5/15/2013',\n",
    " '4/24/2013',\n",
    " '7/17/2013',\n",
    " '11/14/2012',\n",
    " '6/19/2013',\n",
    " '7/17/2013',\n",
    " '7/10/2013',\n",
    " '9/18/2013',\n",
    " '1/25/2017',\n",
    " '9/19/2013',\n",
    " '11/16/2012',\n",
    " '4/22/2013',\n",
    " '11/29/2012',\n",
    " '4/23/2013',\n",
    " '3/13/2013',\n",
    " '3/28/2013',\n",
    " '9/12/2013',\n",
    " '7/31/2013',\n",
    " '6/25/2013',\n",
    " '10/09/2013',\n",
    " '9/11/2013',\n",
    " '8/15/2013',\n",
    " '2/01/2013',\n",
    " '2/27/2013',\n",
    " '8/01/2013',\n",
    " '7/05/2013',\n",
    " '7/03/2013',\n",
    " '8/21/2013',\n",
    " '10/10/2012',\n",
    " '12/06/2012',\n",
    " '12/20/2012',\n",
    " '1/10/2013',\n",
    " '4/29/2013',\n",
    " '1/07/2013',\n",
    " '2/24/2015',\n",
    " '6/29/2015',\n",
    " '10/30/2013',\n",
    " '10/31/2012',\n",
    " '12/07/2015',\n",
    " '8/18/2014',\n",
    " '9/17/2014',\n",
    " '9/16/2016',\n",
    " '1/25/2013',\n",
    " '3/08/2013',\n",
    " '4/15/2013',\n",
    " '3/27/2013',\n",
    " '1/23/2013',\n",
    " '3/19/2013',\n",
    " '11/19/2013',\n",
    " '12/10/2013',\n",
    " '11/03/2015',\n",
    " '4/08/2014',\n",
    " '4/09/2013',\n",
    " '5/28/2013',\n",
    " '6/04/2013',\n",
    " '7/30/2013',\n",
    " '8/20/2013',\n",
    " '9/17/2013',\n",
    " '10/01/2013',\n",
    " '11/26/2013',\n",
    " '10/29/2013',\n",
    " '12/03/2013',\n",
    " '1/14/2014',\n",
    " '4/01/2014',\n",
    " '12/17/2012',\n",
    " '2/16/2016',\n",
    " '8/02/2016',\n",
    " '8/30/2016',\n",
    " '1/03/2013',\n",
    " '4/02/2013',\n",
    " '3/13/2013',\n",
    " '9/24/2014',\n",
    " '4/01/2013',\n",
    " '6/21/2017',\n",
    " '3/09/2016',\n",
    " '12/17/2014',\n",
    " '11/18/2015',\n",
    " '9/29/2015',\n",
    " '11/18/2015',\n",
    " '11/09/2016',\n",
    " '1/05/2017',\n",
    " '8/31/2017']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35797aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_list_cohort = df_global_cohort_baseline['PATNO'].tolist()\n",
    "print('Missing subjects:', [i for i in sub_list if i not in sub_list_cohort])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10996f4d-2d51-4600-a413-1362f5ee3300",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_list_baseline = df_global_cohort_baseline['PATNO'].tolist()\n",
    "sub_list_1y = df_global_1y['PATNO'].tolist()\n",
    "sub_list_2y = df_global_2y['PATNO'].tolist()\n",
    "sub_list_4y = df_global_4y['PATNO'].tolist()\n",
    "\n",
    "common_subs = [sub for sub in sub_list_4y if (sub in sub_list_2y) and (sub in sub_list_1y) and (sub in sub_list_baseline)]\n",
    "print(len(common_subs))\n",
    "#print('Included subjects', len([i for i in sub_list if i in sub_list_4y]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676ec33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_global_cohort_baseline['INCLUDED'] = [sub in sub_list for sub in df_global_cohort_baseline['PATNO']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c59468",
   "metadata": {},
   "outputs": [],
   "source": [
    "ses_ymj = [ses.split('/')[-1] + '-0' + ses.split('/')[0] + '-'+ses.split('/')[1] if len(ses.split('/')[0])==1 else ses.split('/')[-1] + '-' + ses.split('/')[0] + '-' + ses.split('/')[1] for ses in ses_list]\n",
    "df_global_cohort_baseline['SES_YMJ'] = 0\n",
    "df_global_cohort_baseline['SES_YMJ'][df_global_cohort_baseline['INCLUDED']==True] = ses_ymj "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191fb61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_global_cohort_baseline['Correct session'] = [\n",
    "    True if str(df_global_cohort_baseline['INFODT'].tolist()[i]\n",
    "               )[:10] == df_global_cohort_baseline['SES_YMJ'].tolist()[i] else False for i in range(\n",
    "        len(df_global_cohort_baseline))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb5d69a-b269-49fa-bd8a-371ca40f7f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_global_cohort_baseline.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785c6fb3-2f9a-4ded-8d82-4eff63ad44f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_summary['PATNO']= df_global_cohort_baseline['PATNO'].tolist()\n",
    "df_summary['INCLUDED'] =  df_global_cohort_baseline['INCLUDED'].tolist()\n",
    "df_summary.sort_values(['PDXDUR'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdebb3d6-cb9b-4c93-8164-bb1f51d23446",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features = load_ppmi_csv(utils, 'PD_Features-Archived.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82274079-281b-4c80-a9af-34d4b202d689",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features[df_features['PATNO'].isin(df_global_cohort_baseline['PATNO'].tolist())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851a61c0-7025-408c-bc43-7cd756c9529c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_global_cohort_baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9465b93-226b-4210-888d-69376adb139d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_longitudinal_scores.loc[~np.isnan(df_longitudinal_scores['UPDRS_TOT_1Y']) | \\\n",
    "    ~np.isnan(df_longitudinal_scores['UPDRS_TOT_2Y']) | \\\n",
    "    ~np.isnan(df_longitudinal_scores['UPDRS_TOT_4Y'])]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
